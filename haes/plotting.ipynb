{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HA-ES Plotting\n",
    "\n",
    "- udpated version of plotting script used for paper \"...\"\n",
    "\n",
    "## General\n",
    "\n",
    "- imports\n",
    "- defintions\n",
    "- loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygmo as pg\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "from autorank._util import get_sorted_rank_groups\n",
    "from autorank import autorank, plot_stats\n",
    "from tabrepo import load_repository\n",
    "\n",
    "method_id_name_dict = {\n",
    "    \"GES\": \"GES*\",\n",
    "    \"SINGLE_BEST\": \"Single-Best\",\n",
    "    \"QO\": \"QO-ES\",\n",
    "    \"QDO\": \"QDO-ES\",\n",
    "    \"ENS_SIZE_QDO\": \"Ensemble Size\",\n",
    "    \"INFER_TIME_QDO\": \"Inference Time\",\n",
    "    # \"MEMORY_QDO\": \"HAPEns\",\n",
    "    \"MEMORY_QDO\": \"Memory\",\n",
    "    \"DISK_QDO\": \"Diskspace\",\n",
    "}\n",
    "infer_time_weights = np.linspace(0, 1, num=20)[1:]\n",
    "infer_time_weights = np.round(infer_time_weights, 2)\n",
    "multi_ges_method_ids = [f\"MULTI_GES-{time_weight:.2f}\" for time_weight in infer_time_weights]\n",
    "multi_ges_method_names = [f\"Multi-GES({time_weight:.2f})\" for time_weight in infer_time_weights]\n",
    "for id, name in zip(multi_ges_method_ids, multi_ges_method_names):\n",
    "    method_id_name_dict[id] = name\n",
    "\n",
    "print(\"Loading data. This might take a while...\")\n",
    "df = pd.read_csv(\"../data/full.csv\")\n",
    "\n",
    "# Map method IDs to names\n",
    "if \"method\" in df.columns:\n",
    "    df[\"method_name\"] = df[\"method\"].map(method_id_name_dict)\n",
    "else:\n",
    "    raise ValueError(\"Column 'method' not found in DataFrame\")\n",
    "df = df.dropna(subset=['method_name'])\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df[\"method_name\"].unique())\n",
    "print(df[\"method\"].unique())\n",
    "\n",
    "df[\"models_used_length\"] = df[\"models_used\"].apply(len)\n",
    "\n",
    "if False:\n",
    "    print(\"Filtering methods...\")\n",
    "    filter_methods = [\n",
    "        \"Single-Best\",\n",
    "        \"GES*\",\n",
    "        \"QDO-ES\",\n",
    "        # \"HAPEns\",\n",
    "        \"Memory\",\n",
    "        \"Ensemble Size\",\n",
    "        \"Inference Time\",\n",
    "        \"Diskspace\",\n",
    "        \"Multi-GES(0.21)\",\n",
    "        \"Multi-GES(0.79)\"\n",
    "    ]\n",
    "    df = df[df['method_name'].isin(filter_methods)]\n",
    "    print(df.shape)\n",
    "    print(df.columns)\n",
    "    print(df[\"method_name\"].unique())\n",
    "    print(df[\"method\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "### Boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def extract_numeric_part(method_name):\n",
    "    \"\"\"\n",
    "    Extracts the numeric part from a method name string. If no numeric part is found, returns None.\n",
    "    \"\"\"\n",
    "    if isinstance(method_name, str):\n",
    "        match = re.search(r\"\\((\\d*\\.?\\d+)\\)\", method_name)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "    return None\n",
    "\n",
    "def boxplot(\n",
    "    df: pd.DataFrame,\n",
    "    y_str: str,\n",
    "    log_y_scale: bool = False,\n",
    "    log_x_scale: bool = False,\n",
    "    flip_y_axis: bool = False,\n",
    "    orient: str = \"v\",\n",
    "    rotation_x_ticks: int = 45,\n",
    "    outliers=False,\n",
    "    sort_by_median: bool = True,  # <-- added flag\n",
    "):\n",
    "    if y_str not in df.columns:\n",
    "        raise ValueError(f\"Column '{y_str}' not found in DataFrame\")\n",
    "    \n",
    "    df[\"method_name\"] = df[\"method_name\"].astype(str)\n",
    "\n",
    "    if sort_by_median:\n",
    "        # Sort by median of the target column\n",
    "        medians = (\n",
    "            df.groupby(\"method_name\")[y_str]\n",
    "            .median()\n",
    "            .sort_values(ascending=False)  # largest median first\n",
    "        )\n",
    "        df[\"method_name\"] = pd.Categorical(df[\"method_name\"], categories=medians.index, ordered=True)\n",
    "        df = df.sort_values(\"method_name\")\n",
    "    else:\n",
    "        # Alphabetic + numeric sorting as fallback\n",
    "        df[\"alphabetic\"] = df[\"method_name\"].apply(lambda x: re.split(r\"\\(\\d*\\.?\\d+\\)\", x)[0])\n",
    "        df[\"numeric\"] = df[\"method_name\"].apply(extract_numeric_part)\n",
    "        df = df.sort_values(by=[\"alphabetic\", \"numeric\"], ascending=[True, True])\n",
    "        df = df.drop(columns=[\"alphabetic\", \"numeric\"])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if orient == \"v\":\n",
    "        sns.boxplot(\n",
    "            data=df,\n",
    "            x=\"method_name\",\n",
    "            y=y_str,\n",
    "            hue=\"method_name\",\n",
    "            palette=\"pastel\",\n",
    "            linewidth=2,\n",
    "            orient=orient,\n",
    "            legend=False,\n",
    "            showfliers=outliers,\n",
    "        )\n",
    "    elif orient == \"h\":\n",
    "        sns.boxplot(\n",
    "            data=df,\n",
    "            x=y_str,\n",
    "            y=\"method_name\",\n",
    "            hue=\"method_name\",\n",
    "            palette=\"pastel\",\n",
    "            linewidth=2,\n",
    "            orient=orient,\n",
    "            legend=False,\n",
    "            showfliers=outliers,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Orient '{orient}' not supported\")\n",
    "\n",
    "    if orient == \"v\":\n",
    "        plt.ylabel(y_str)\n",
    "        plt.xlabel(\"Ensemble Method\")\n",
    "    else:\n",
    "        plt.xlabel(y_str)\n",
    "        plt.ylabel(\"Ensemble Method\")\n",
    "\n",
    "    if log_y_scale:\n",
    "        plt.yscale(\"log\")\n",
    "    if log_x_scale:\n",
    "        plt.xscale(\"log\")\n",
    "    if flip_y_axis:\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.xticks(rotation=rotation_x_ticks)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    directory = \"../plots\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    plt.savefig(f\"{directory}/boxplot_{y_str}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(f\"{directory}/boxplot_{y_str}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def interactive_boxplot(df, column_options):\n",
    "    y_str_widget = widgets.Dropdown(\n",
    "        options=column_options,\n",
    "        description=\"Y-axis column:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    log_y_scale_widget = widgets.Checkbox(value=False, description=\"Log-scale Y-axis\")\n",
    "    log_x_scale_widget = widgets.Checkbox(value=False, description=\"Log-scale X-axis\")\n",
    "    flip_y_axis_widget = widgets.Checkbox(value=False, description=\"Flip Y-axis\")\n",
    "    orient_widget = widgets.RadioButtons(\n",
    "        options=[\"v\", \"h\"],\n",
    "        value=\"h\",\n",
    "        description=\"Orientation:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    rotation_x_ticks_widget = widgets.IntSlider(\n",
    "        value=0, min=0, max=90, step=5, description=\"Rotation X-ticks\"\n",
    "    )\n",
    "\n",
    "    ui = widgets.VBox(\n",
    "        [\n",
    "            y_str_widget,\n",
    "            log_y_scale_widget,\n",
    "            log_x_scale_widget,\n",
    "            flip_y_axis_widget,\n",
    "            orient_widget,\n",
    "            rotation_x_ticks_widget,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    out = widgets.interactive_output(\n",
    "        boxplot,\n",
    "        {\n",
    "            \"df\": widgets.fixed(df),\n",
    "            \"y_str\": y_str_widget,\n",
    "            \"log_y_scale\": log_y_scale_widget,\n",
    "            \"log_x_scale\": log_x_scale_widget,\n",
    "            \"flip_y_axis\": flip_y_axis_widget,\n",
    "            \"orient\": orient_widget,\n",
    "            \"rotation_x_ticks\": rotation_x_ticks_widget,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical Difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd_evaluation(\n",
    "    hypervolumes,\n",
    "    maximize_metric=True,\n",
    "    plt_title=\"Critical Difference Plot\",\n",
    "    filename=\"CriticalDifferencePlot.png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    hypervolumes: DataFrame with method names as columns and tasks as rows, each cell contains hypervolume.\n",
    "    maximize_metric: Boolean, True if higher values are better.\n",
    "    output_path: Where to save the plot, if None, plot will not be saved.\n",
    "    plt_title: Title of the plot.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    rank_data = -hypervolumes if maximize_metric else hypervolumes\n",
    "\n",
    "    # Run autorank\n",
    "    result = autorank(rank_data, alpha=0.05, verbose=False, order=\"ascending\")\n",
    "\n",
    "    # Plot with updated font size\n",
    "    plt.close(\"all\")\n",
    "    width = 6\n",
    "    fig, ax = plt.subplots(figsize=(12, width))\n",
    "    plt.rcParams.update({\"font.size\": 20})\n",
    "\n",
    "    plot_stats(result, ax=ax)\n",
    "    ax.tick_params(axis=\"both\", labelsize=20)  # Set font size for axis ticks\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    ax.set_xticklabels(labels, fontsize=20)  # Adjust fontsize as needed\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(filename, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_options = [\"models_used_length\", \"normalized_roc_auc_test\", \"normalized_roc_auc_val\", \"inference_time\", \"memory\", \"diskspace\", \"normalized_memory\"]\n",
    "\n",
    "#filtered_df = df[~df['method_name'].isin(['Multi-GES(0.43)', 'Multi-GES(0.50)'])].copy()\n",
    "interactive_boxplot(df, column_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mticker\n",
    "# Group the DataFrame by 'method_name' and compute total and unique counts\n",
    "result_df = df.groupby('method_name').agg(\n",
    "    total_solutions=('models_used', 'count'),\n",
    "    unique_solutions=('models_used', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate the percentage of unique solutions\n",
    "result_df['percentage_unique_solutions'] = (\n",
    "    result_df['unique_solutions'] / result_df['total_solutions'] * 100\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by the percentage of unique solutions for clearer visualization\n",
    "result_df = result_df.sort_values('percentage_unique_solutions', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "color_total = 'steelblue'\n",
    "color_unique = 'indianred'\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.bar(result_df['method_name'], result_df['total_solutions'], color=color_total, label='Total Solutions')\n",
    "\n",
    "# --- Y-axis 1: Total Solutions ---\n",
    "ax.set_ylabel(\"Total Solutions\", color='black', fontsize=20, labelpad=15)\n",
    "ax.tick_params(axis='y', labelcolor='black', pad=5, labelsize=18)\n",
    "ax.grid(axis='y', linestyle='--', color=color_total, alpha=0.4)\n",
    "ax.yaxis.set_major_locator(mticker.MaxNLocator(nbins=6, integer=True))\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(result_df['method_name'], result_df['percentage_unique_solutions'], color=color_unique, marker='o', linestyle='-', linewidth=3, markersize=10, label='Percentage Unique Solutions')\n",
    "\n",
    "# --- Y-axis 2: Percentage Unique ---\n",
    "ax2.set_ylabel('Percentage of Unique Solutions (%)', color='black', fontsize=20, labelpad=15)\n",
    "ax2.tick_params(axis='y', labelcolor='black', pad=5, labelsize=18)\n",
    "ax2.grid(False)\n",
    "ax2.yaxis.set_major_locator(mticker.MaxNLocator(nbins=6))\n",
    "ax2.yaxis.set_major_formatter(mticker.PercentFormatter())\n",
    "\n",
    "# --- Common Settings ---\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=18)\n",
    "\n",
    "# --- Legend ---\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc='lower left', fontsize=16)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto Front Eval\n",
    "\n",
    "### True Pareto Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParetoFront(df, objectives, return_mask=False):\n",
    "    \"\"\"\n",
    "    Identify the Pareto-efficient points in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - objectives: list of objective column names.\n",
    "    - return_mask: if True, return a boolean mask; else return indices.\n",
    "\n",
    "    Returns:\n",
    "    - Boolean mask or indices of Pareto-efficient points.\n",
    "    \"\"\"\n",
    "    data = df[objectives].values\n",
    "    is_efficient = np.ones(data.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(data):\n",
    "        if is_efficient[i]:\n",
    "            is_efficient[is_efficient] = np.any(data[is_efficient] < c, axis=1)  # Keep any point with a lower value\n",
    "            is_efficient[i] = True  # And keep self\n",
    "    if return_mask:\n",
    "        return is_efficient\n",
    "    else:\n",
    "        return np.where(is_efficient)[0]\n",
    "    \n",
    "def calculate_pareto_fronts(df, objectives):\n",
    "    \"\"\"\n",
    "    Calculate the Pareto front points for all solutions in the DataFrame per task_id, seed, and fold.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - objectives: list of two objective column names.\n",
    "\n",
    "    Returns:\n",
    "    - A nested dictionary with structure {task_id: {seed: {fold: pareto_front_df}}}\n",
    "    \"\"\"\n",
    "    if len(objectives) != 2:\n",
    "        raise ValueError(\"Exactly two objectives must be provided.\")\n",
    "\n",
    "    # Initialize a dictionary to store the Pareto fronts\n",
    "    pareto_fronts = {}\n",
    "\n",
    "    # Iterate over unique task_ids\n",
    "    for task_id in df[\"task_id\"].unique():\n",
    "        pareto_fronts[task_id] = {}\n",
    "\n",
    "        # Iterate over unique seeds\n",
    "        for seed in df[\"seed\"].unique():\n",
    "            pareto_fronts[task_id][seed] = {}\n",
    "\n",
    "            # Iterate over unique folds\n",
    "            for fold in df[\"fold\"].unique():\n",
    "                # Filter the DataFrame for the current task_id, seed, and fold\n",
    "                df_fold = df[\n",
    "                    (df[\"task_id\"] == task_id)\n",
    "                    & (df[\"seed\"] == seed)\n",
    "                    & (df[\"fold\"] == fold)\n",
    "                ]\n",
    "\n",
    "                if df_fold.empty:\n",
    "                    continue  # Skip if no data for this combination\n",
    "\n",
    "                # Compute Pareto front for df_fold\n",
    "                is_efficient = getParetoFront(df_fold, objectives, return_mask=True)\n",
    "                pareto_front_df = df_fold[is_efficient]\n",
    "\n",
    "                # Store the Pareto front DataFrame\n",
    "                pareto_fronts[task_id][seed][fold] = pareto_front_df\n",
    "\n",
    "    return pareto_fronts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_data import normalize_data\n",
    "\n",
    "hw_score = 'hw_score'\n",
    "perf_metric = 'normalized_roc_auc_test'\n",
    "hw_metrics = ['normalized_time', 'normalized_memory', 'normalized_diskspace']\n",
    "\n",
    "df['normalized_hw_score'] = df[hw_metrics].mean(axis=1)\n",
    "for task in df[\"task\"].unique():\n",
    "    mask = df[\"task\"] == task\n",
    "    if \"hw_score\" in df.columns:\n",
    "        df.loc[mask, \"normalized_hw_score\"] = normalize_data(\n",
    "            df.loc[mask, \"hw_score\"]\n",
    "        )\n",
    "hw_metrics.append('normalized_hw_score')\n",
    "\n",
    "metric_pairs = [(perf_metric, hw_metric) for hw_metric in hw_metrics]\n",
    "print(metric_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypervolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pareto_efficient(costs, return_mask=True):\n",
    "    is_efficient = np.ones(costs.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(costs):\n",
    "        is_efficient[i] = not np.any(\n",
    "            np.all(costs <= c, axis=1) & np.any(costs < c, axis=1)\n",
    "        )\n",
    "    return is_efficient if return_mask else costs[is_efficient]\n",
    "\n",
    "\n",
    "def calculate_average_hypervolumes(df, method_name, objectives: list):\n",
    "    if len(objectives) != 2: \n",
    "        return\n",
    "    df_method = df[df[\"method_name\"] == method_name]\n",
    "    hypervolumes = {}\n",
    "\n",
    "    # Iterate over unique task_ids\n",
    "    for task_id in df_method[\"task_id\"].unique():\n",
    "        seed_hypervolumes = []  # Store hypervolumes for each seed\n",
    "\n",
    "        for seed in df_method[\"seed\"].unique():\n",
    "            fold_hypervolumes = []  # Store hypervolumes for each fold under the current seed\n",
    "\n",
    "            for fold in df_method[\"fold\"].unique():\n",
    "                df_fold = df_method[\n",
    "                    (df_method[\"task_id\"] == task_id)\n",
    "                    & (df_method[\"seed\"] == seed)\n",
    "                    & (df_method[\"fold\"] == fold)\n",
    "                ]\n",
    "\n",
    "                # Use a different variable name here\n",
    "                objective_values = np.array(\n",
    "                    [\n",
    "                        df_fold[objectives[0]].values,\n",
    "                        df_fold[objectives[1]].values,\n",
    "                    ]\n",
    "                ).T\n",
    "                is_efficient = is_pareto_efficient(objective_values)\n",
    "                efficient_objectives = objective_values[is_efficient]\n",
    "\n",
    "                ref_point = [\n",
    "                    1.01,\n",
    "                    1.01,\n",
    "                ]  # Reference point beyond the worst values of objectives\n",
    "                hv = pg.hypervolume(efficient_objectives)\n",
    "                hypervolume = hv.compute(ref_point)\n",
    "                fold_hypervolumes.append(hypervolume)\n",
    "\n",
    "            # Average hypervolumes across all folds for a given seed\n",
    "            if fold_hypervolumes:\n",
    "                average_fold_hypervolume = np.mean(fold_hypervolumes)\n",
    "                seed_hypervolumes.append(average_fold_hypervolume)\n",
    "\n",
    "        # Average the averaged fold hypervolumes across seeds\n",
    "        if seed_hypervolumes:\n",
    "            average_seed_hypervolume = np.mean(seed_hypervolumes)\n",
    "            hypervolumes[task_id] = average_seed_hypervolume\n",
    "\n",
    "    return hypervolumes\n",
    "\n",
    "\n",
    "def find_non_dominated(points):\n",
    "    \"\"\"Identify the indices of non-dominated points.\"\"\"\n",
    "    is_efficient = np.ones(points.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(points):\n",
    "        is_efficient[i] = not np.any(\n",
    "            np.all(points <= c, axis=1) & np.any(points < c, axis=1)\n",
    "        )\n",
    "    return np.where(is_efficient)[0]\n",
    "\n",
    "\n",
    "def plot_hypervolumes(all_hypervolumes, title: str, directory: str = \"../plots/\"):\n",
    "    # Prepare the data for plotting\n",
    "    methods = list(all_hypervolumes.keys())  # Method names\n",
    "    hv_values = [list(all_hypervolumes[method].values()) for method in methods]\n",
    "    data = []\n",
    "\n",
    "    # Creating a DataFrame suitable for Seaborn\n",
    "    for method_index, values in enumerate(hv_values):\n",
    "        for value in values:\n",
    "            data.append({\"Method\": methods[method_index], \"Hypervolume\": value})\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Calculate medians for each method and sort by median\n",
    "    median_order = df.groupby(\"Method\")[\"Hypervolume\"].median().sort_values(ascending=False).index\n",
    "\n",
    "    # Set the figure size and style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Use seaborn's boxplot to plot the DataFrame, ordered by median values\n",
    "    ax = sns.boxplot(\n",
    "        y=\"Method\", x=\"Hypervolume\", data=df, hue=\"Method\", palette=\"Set2\",\n",
    "        orient=\"h\", order=median_order\n",
    "    )\n",
    "\n",
    "    # Set titles and labels\n",
    "    ax.set_ylabel(\"Method\", fontsize=20)\n",
    "    ax.set_xlabel(\"Hypervolume\", fontsize=20)\n",
    "\n",
    "    # Set font size for ticks\n",
    "    ax.tick_params(axis=\"x\", labelrotation=45, labelsize=16)\n",
    "    ax.tick_params(axis=\"y\", labelsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(directory + title + \".png\", dpi=300)\n",
    "    plt.savefig(directory + title + \".pdf\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for perf_metric, hw_metric in metric_pairs:\n",
    "    metric_pair = (perf_metric, hw_metric)\n",
    "    print(f\"\\n--- Processing pair: {metric_pair} ---\")\n",
    "\n",
    "    methods = df[\"method_name\"].unique()\n",
    "    all_hypervolumes = {}\n",
    "\n",
    "    for method in methods:\n",
    "        all_hypervolumes[method] = calculate_average_hypervolumes(df, method, metric_pair)\n",
    "\n",
    "    # You might want to make this plot function also save to a dynamic filename\n",
    "    plot_hypervolumes(all_hypervolumes, f\"bp_hv_{perf_metric}_{hw_metric}\")\n",
    "\n",
    "    # Create dynamic filenames to avoid overwriting results\n",
    "    csv_filename = f\"../data/hypervolumes_{perf_metric}_{hw_metric}.csv\"\n",
    "    plot_filename_pdf = f\"../plots/CDP_HV_{perf_metric}_{hw_metric}.pdf\"\n",
    "    plot_filename_png = f\"../plots/CDP_HV_{perf_metric}_{hw_metric}.png\"\n",
    "\n",
    "    hypervolumes_df = pd.DataFrame(all_hypervolumes)\n",
    "    hypervolumes_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Saved hypervolume data to {csv_filename}\")\n",
    "\n",
    "    data = []\n",
    "    for method, task_ids in all_hypervolumes.items():\n",
    "        for task_id, hypervolume in task_ids.items():\n",
    "            data.append({\"Task\": task_id, \"Method\": method, \"Hypervolume\": hypervolume})\n",
    "\n",
    "    df_hypervolumes = pd.DataFrame(data)\n",
    "    pivot_hypervolumes = df_hypervolumes.pivot(\n",
    "        index=\"Task\", columns=\"Method\", values=\"Hypervolume\"\n",
    "    )\n",
    "\n",
    "    # Run CD evaluation with dynamic filenames\n",
    "    hv_result = cd_evaluation(\n",
    "        pivot_hypervolumes,\n",
    "        maximize_metric=True,\n",
    "        plt_title=f\"Hypervolume CD Plot for {perf_metric} vs {hw_metric}\",\n",
    "        filename=plot_filename_pdf,\n",
    "    )\n",
    "    hv_result = cd_evaluation(\n",
    "        pivot_hypervolumes,\n",
    "        maximize_metric=True,\n",
    "        plt_title=f\"Hypervolume CD Plot for {perf_metric} vs {hw_metric}\",\n",
    "        filename=plot_filename_png,\n",
    "    )\n",
    "    print(f\"Saved critical difference plots for {metric_pair}\")\n",
    "    # print(hv_result)\n",
    "\n",
    "print(\"\\n--- Analysis complete for all pairs. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Generational Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.indicators.igd import IGD\n",
    "\n",
    "def calculate_IGD_per_method(df, objectives):\n",
    "    \"\"\"\n",
    "    Calculate the IGD for each method_name per task_id, seed, and fold.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - objectives: list of two objective column names.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with columns [task_id, seed, fold, method_name, IGD].\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Iterate over unique task_ids\n",
    "    for task_id in df[\"task_id\"].unique():\n",
    "        # Iterate over unique seeds\n",
    "        for seed in df[\"seed\"].unique():\n",
    "            # Iterate over unique folds\n",
    "            for fold in df[\"fold\"].unique():\n",
    "                # Filter the DataFrame for the current task_id, seed, and fold\n",
    "                df_fold = df[\n",
    "                    (df[\"task_id\"] == task_id)\n",
    "                    & (df[\"seed\"] == seed)\n",
    "                    & (df[\"fold\"] == fold)\n",
    "                ]\n",
    "\n",
    "                if df_fold.empty:\n",
    "                    continue  # Skip if no data for this combination\n",
    "\n",
    "                # Compute the reference Pareto front (from all methods)\n",
    "                is_efficient = getParetoFront(df_fold, objectives, return_mask=True)\n",
    "                reference_pareto_front = df_fold[is_efficient]\n",
    "                pf_points = reference_pareto_front[objectives].values\n",
    "\n",
    "                # Iterate over unique method_names\n",
    "                for method_name in df_fold[\"method_name\"].unique():\n",
    "                    df_method = df_fold[df_fold[\"method_name\"] == method_name]\n",
    "\n",
    "                    # Compute Pareto front for df_method\n",
    "                    is_efficient = getParetoFront(df_method, objectives, return_mask=True)\n",
    "                    method_pareto_front = df_method[is_efficient]\n",
    "                    method_points = method_pareto_front[objectives].values\n",
    "\n",
    "                    if len(method_points) == 0 or len(pf_points) == 0:\n",
    "                        igd_value = np.nan\n",
    "                    else:\n",
    "                        # Calculate IGD\n",
    "                        ind = IGD(pf_points)\n",
    "                        igd_value = ind(method_points)\n",
    "\n",
    "                    # Append results\n",
    "                    results.append({\n",
    "                        \"task_id\": task_id,\n",
    "                        \"seed\": seed,\n",
    "                        \"fold\": fold,\n",
    "                        \"method_name\": method_name,\n",
    "                        \"IGD\": igd_value\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IGD per method\n",
    "for perf_metric, hw_metric in metric_pairs:\n",
    "    metric_pair = [perf_metric, hw_metric]\n",
    "    igd_results = calculate_IGD_per_method(df, metric_pair)\n",
    "    \n",
    "    # Group by 'method_name' and 'task_id', and aggregate with 'sum'\n",
    "    agg_igd = igd_results.groupby([\"method_name\", \"task_id\"]).agg('sum').reset_index()\n",
    "    igd_pivots = agg_igd.pivot(\n",
    "        index=\"task_id\", columns=\"method_name\", values=\"IGD\"\n",
    "    )\n",
    "\n",
    "    igd_result = cd_evaluation(\n",
    "        igd_pivots,\n",
    "        maximize_metric=False,\n",
    "        plt_title=\"Hypervolume Critical Difference Plot\",\n",
    "        filename=f\"../plots/CDP_IGD_{perf_metric}_{hw_metric}.png\",\n",
    "    )\n",
    "    igd_result = cd_evaluation(\n",
    "        igd_pivots,\n",
    "        maximize_metric=False,\n",
    "        plt_title=\"Hypervolume Critical Difference Plot\",\n",
    "        filename=f\"../plots/CDP_IGD_{perf_metric}_{hw_metric}.pdf\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto Front Topography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique method names\n",
    "method_names = df[\"method_name\"].unique()\n",
    "\n",
    "# Calculate Pareto fronts for each method\n",
    "method_pareto_fronts = {}\n",
    "for method in method_names:\n",
    "    df_method = df[df[\"method_name\"] == method]\n",
    "    method_pareto_fronts[method] = calculate_pareto_fronts(df_method, ['normalized_roc_auc_test', 'normalized_time'])\n",
    "\n",
    "# Function to count the number of solutions in Pareto fronts\n",
    "def count_pareto_solutions(pareto_fronts):\n",
    "    counts = {}\n",
    "    for task_id in pareto_fronts:\n",
    "        for seed in pareto_fronts[task_id]:\n",
    "            for fold in pareto_fronts[task_id][seed]:\n",
    "                pareto_front_df = pareto_fronts[task_id][seed][fold]\n",
    "                n_solutions = len(pareto_front_df)\n",
    "                key = (task_id, seed, fold)\n",
    "                counts[key] = n_solutions\n",
    "    return counts\n",
    "\n",
    "# Count solutions in the method-specific Pareto fronts\n",
    "method_pareto_counts = {}\n",
    "for method in method_names:\n",
    "    method_pareto_counts[method] = count_pareto_solutions(method_pareto_fronts[method])\n",
    "\n",
    "# Calculate total number of solutions per method, task_id, seed, fold\n",
    "total_counts = df.groupby(['method_name', 'task_id', 'seed', 'fold']).size().reset_index(name='total_solutions')\n",
    "\n",
    "# Prepare data for plotting counts\n",
    "data_list = []\n",
    "\n",
    "# Add the method-specific Pareto fronts and total counts\n",
    "for method in method_names:\n",
    "    counts = method_pareto_counts[method]\n",
    "    for key, n_solutions in counts.items():\n",
    "        task_id, seed, fold = key\n",
    "        # Get total number of solutions for this method, task_id, seed, fold\n",
    "        total_solutions = total_counts[\n",
    "            (total_counts['method_name'] == method) &\n",
    "            (total_counts['task_id'] == task_id) &\n",
    "            (total_counts['seed'] == seed) &\n",
    "            (total_counts['fold'] == fold)\n",
    "        ]['total_solutions'].values[0]\n",
    "        data_list.append({\n",
    "            'method_name': method,\n",
    "            'task_id': task_id,\n",
    "            'seed': seed,\n",
    "            'fold': fold,\n",
    "            'n_pareto_solutions': n_solutions,\n",
    "            'total_solutions': total_solutions\n",
    "        })\n",
    "\n",
    "pareto_counts_df = pd.DataFrame(data_list)\n",
    "pareto_counts_df['proportion_pareto'] = pareto_counts_df['n_pareto_solutions'] / pareto_counts_df['total_solutions']\n",
    "\n",
    "\n",
    "# Calculate the average number of Pareto solutions per method\n",
    "avg_pareto_solutions = pareto_counts_df.groupby('method_name')['n_pareto_solutions'].mean().reset_index()\n",
    "\n",
    "# Calculate the average total number of solutions per method\n",
    "avg_total_solutions = pareto_counts_df.groupby('method_name')['total_solutions'].mean().reset_index()\n",
    "\n",
    "# Merge the averages into a single DataFrame\n",
    "avg_counts_df = pd.merge(avg_pareto_solutions, avg_total_solutions, on='method_name')\n",
    "\n",
    "# Sort methods by average total solutions for better visualization\n",
    "avg_counts_df = avg_counts_df.sort_values('total_solutions')\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=False)\n",
    "\n",
    "# Plot average number of Pareto solutions\n",
    "sns.barplot(\n",
    "    data=avg_counts_df,\n",
    "    x='method_name',\n",
    "    y='n_pareto_solutions',\n",
    "    ax=axes[0],\n",
    "    palette='Blues_d'\n",
    ")\n",
    "axes[0].set_title('Average Number of Pareto Solutions per Method')\n",
    "axes[0].set_xlabel('Method Name')\n",
    "axes[0].set_ylabel('Average Number of Pareto Solutions')\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Plot average total number of solutions\n",
    "sns.barplot(\n",
    "    data=avg_counts_df,\n",
    "    x='method_name',\n",
    "    y='total_solutions',\n",
    "    ax=axes[1],\n",
    "    palette='Greens_d'\n",
    ")\n",
    "axes[1].set_title('Average Total Number of Solutions per Method')\n",
    "axes[1].set_xlabel('Method Name')\n",
    "axes[1].set_ylabel('Average Total Number of Solutions')\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of Solutions on the Global Pareto Front\n",
    "\n",
    "The following plot illustrates the effectiveness of each method in generating solutions that are part of the global Pareto front. For each task, the global Pareto front is determined by considering all solutions from all methods. We then calculate the proportion of each method's generated solutions that lie on this global front. A higher proportion indicates that a method is more efficient at finding globally optimal solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "print(\"Preparing data for per-task and per-seed analysis...\")\n",
    "df_main = df.copy()\n",
    "task_column_name = 'task'\n",
    "\n",
    "cost_metrics = ['normalized_time', 'normalized_memory', 'normalized_diskspace']\n",
    "unique_tasks = df_main[task_column_name].unique()\n",
    "unique_seeds = df_main['seed'].unique()\n",
    "all_proportions = []\n",
    "\n",
    "# Loop over each task\n",
    "for task in unique_tasks:\n",
    "    # --- CHANGE: Added inner loop to iterate over each seed ---\n",
    "    for seed in unique_seeds:\n",
    "        # Filter for the specific task and seed combination\n",
    "        df_task_seed = df_main[(df_main[task_column_name] == task) & (df_main['seed'] == seed)].copy()\n",
    "\n",
    "        # If there's no data for this combination, skip to the next\n",
    "        if df_task_seed.empty:\n",
    "            continue\n",
    "\n",
    "        # Get total solutions for this specific task-seed combo\n",
    "        total_solutions_task_seed = df_task_seed.groupby('method_name', observed=True).size()\n",
    "\n",
    "        for metric in cost_metrics:\n",
    "            objectives = ['normalized_roc_auc_test', metric]\n",
    "            pf_col_name = f'is_on_pf_{metric}'\n",
    "            df_task_seed[pf_col_name] = False\n",
    "\n",
    "            # Calculate Pareto front for each fold within this task and seed\n",
    "            global_pareto_indices = []\n",
    "            # --- CHANGE: Grouping is now only by 'fold' ---\n",
    "            for name, group in df_task_seed.groupby(['fold']):\n",
    "                if not group.empty:\n",
    "                    is_efficient_mask = getParetoFront(group, objectives, return_mask=True)\n",
    "                    global_pareto_indices.extend(group[is_efficient_mask].index)\n",
    "\n",
    "            # Ensure we only use unique indices\n",
    "            df_task_seed.loc[list(set(global_pareto_indices)), pf_col_name] = True\n",
    "\n",
    "            # Calculate proportions for each method within this task-seed combo\n",
    "            solutions_on_pf = df_task_seed.groupby('method_name', observed=True)[pf_col_name].sum()\n",
    "            task_seed_proportions = (solutions_on_pf / total_solutions_task_seed).fillna(0)\n",
    "\n",
    "            # Store the results for this task-seed combo\n",
    "            for method_name, proportion in task_seed_proportions.items():\n",
    "                all_proportions.append({\n",
    "                    task_column_name: task,\n",
    "                    'seed': seed, # Also store the seed\n",
    "                    'method_name': method_name,\n",
    "                    'metric_type': metric.replace('_', ' ').replace('normalized', '').strip().capitalize(),\n",
    "                    'proportion': proportion,\n",
    "                    'num_solutions': total_solutions_task_seed.get(method_name, 0)\n",
    "                })\n",
    "\n",
    "print(\"Averaging the per-task-seed results...\")\n",
    "per_task_seed_df = pd.DataFrame(all_proportions)\n",
    "\n",
    "# Filter out the 'Single-Best' method if it exists\n",
    "if 'Single-Best' in per_task_seed_df['method_name'].unique():\n",
    "    per_task_seed_df = per_task_seed_df[per_task_seed_df['method_name'] != 'Single-Best']\n",
    "\n",
    "# Forcefully remove the unused 'Single-Best' category to fix plotting issues\n",
    "if isinstance(per_task_seed_df['method_name'].dtype, pd.CategoricalDtype):\n",
    "    per_task_seed_df['method_name'] = per_task_seed_df['method_name'].cat.remove_unused_categories()\n",
    "\n",
    "# Determine the final sort order based on the new, more granular mean\n",
    "sort_order_df = per_task_seed_df.groupby('method_name', observed=True)['proportion'].mean().sort_values(ascending=False)\n",
    "sort_order = sort_order_df.index.tolist()\n",
    "# Note: n is now the average number of solutions per task-seed combination\n",
    "avg_solutions_per_method = per_task_seed_df.groupby('method_name')['num_solutions'].mean()\n",
    "\n",
    "print(\"Generating the final plot...\")\n",
    "plt.figure(figsize=(14, 10))\n",
    "ax = sns.barplot(\n",
    "    data=per_task_seed_df,\n",
    "    y='method_name',\n",
    "    x='proportion',\n",
    "    hue='metric_type',\n",
    "    order=sort_order,\n",
    "    palette='viridis',\n",
    "    errorbar='se'\n",
    ")\n",
    "\n",
    "new_yticklabels = []\n",
    "for method in sort_order:\n",
    "    avg_sol = avg_solutions_per_method.get(method, 0)\n",
    "    # The label now reflects that 'n' is the avg per task-seed run\n",
    "    new_yticklabels.append(f\"{method}\\n(n={avg_sol:.1f})\")\n",
    "\n",
    "ax.set_yticklabels(new_yticklabels)\n",
    "\n",
    "# Apply formatting\n",
    "ax.set_xlabel('Average Per-Task Ensemble Proportion on Global Pareto Front', fontsize=18)\n",
    "ax.set_ylabel('')\n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.legend(title='Cost Metric', fontsize=14, title_fontsize=16)\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "directory = \"../plots\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "plt.savefig(f\"{directory}/proportion_on_global_pf_per_task_seed_avg.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(f\"{directory}/proportion_on_global_pf_per_task_seed_avg.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Construction Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives_to_plot = ['normalized_roc_auc_test', 'normalized_time']\n",
    "\n",
    "# --- Performance: Subsample the data ---\n",
    "n_samples = 3000000\n",
    "if len(df) > n_samples:\n",
    "    print(f\"Dataset is large. Using a random sample of {n_samples} points for speed.\")\n",
    "    df_sample = df.sample(n=n_samples, random_state=42)\n",
    "else:\n",
    "    df_sample = df\n",
    "\n",
    "# --- Subplot Grid Setup ---\n",
    "method_names = sorted(df_sample['method_name'].unique()) # Sort names for consistent order\n",
    "num_methods = len(method_names)\n",
    "\n",
    "grid_cols = math.ceil(math.sqrt(num_methods))\n",
    "grid_rows = math.ceil(num_methods / grid_cols)\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, axes = plt.subplots(\n",
    "    grid_rows, \n",
    "    grid_cols, \n",
    "    figsize=(20, 18), \n",
    "    sharex=True, \n",
    "    sharey=True\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "print(\"Generating final polished density plots...\")\n",
    "\n",
    "# --- Loop through each method and create its subplot ---\n",
    "for i, method in enumerate(method_names):\n",
    "    ax = axes[i]\n",
    "    df_method_sample = df_sample[df_sample['method_name'] == method]\n",
    "    \n",
    "    sns.kdeplot(\n",
    "        data=df_method_sample,\n",
    "        x=objectives_to_plot[0],\n",
    "        y=objectives_to_plot[1],\n",
    "        fill=True,\n",
    "        alpha=0.5,\n",
    "        color='c',\n",
    "        levels=7,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    sns.kdeplot(\n",
    "        data=df_method_sample,\n",
    "        x=objectives_to_plot[0],\n",
    "        y=objectives_to_plot[1],\n",
    "        fill=False,\n",
    "        linewidths=1.0,\n",
    "        color='teal',\n",
    "        levels=7,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(method, fontsize=18, pad=12)\n",
    "\n",
    "# --- Final Touches ---\n",
    "for i in range(num_methods, len(axes)):\n",
    "    ax = axes[i]\n",
    "    ax.set_frame_on(False) # Make frame invisible\n",
    "    ax.get_yaxis().set_visible(False) # Hide Y axis objects\n",
    "    ax.spines['bottom'].set_visible(False) # Hide the x-axis line\n",
    "    ax.tick_params(bottom=False) # Hide the x-axis tick marks\n",
    "\n",
    "# Set common labels and a title for the entire figure\n",
    "fig.supxlabel('Normalized Test ROC AUC', fontsize=22, y=0.07)\n",
    "fig.supylabel('Normalized Inference Time', fontsize=22, x=0.08)\n",
    "fig.suptitle('Density of Ensembles Constructed by Method', fontsize=28, y=0.97)\n",
    "\n",
    "# --- CHANGE: Add buffer to xlim/ylim to fix overlapping zero labels ---\n",
    "plt.setp(axes, xlim=(-0.02, 1.02), ylim=(-0.02, 1.02))\n",
    "\n",
    "# Increase tick label size and clear individual labels\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "\n",
    "# Adjust layout to prevent titles and labels from overlapping\n",
    "plt.tight_layout(rect=[0.09, 0.08, 1, 0.94])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latex Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latex_table(df, repo, filename=\"table.tex\", max_char=15):\n",
    "    methods = df[\"method_name\"].unique()\n",
    "    task_ids = df[\"task_id\"].unique()\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{longtable}{l\" + \"c\" * len(methods) + \"}\\n\")\n",
    "        f.write(\n",
    "            \"\\\\caption{Test ROC AUC - Binary: The mean and standard deviation of the test score over all folds for each method. The best methods per dataset are shown in bold. All methods close to the best method are considered best (using NumPyâ€™s default \\\\texttt{isclose} function).}\\n\"\n",
    "        )\n",
    "        f.write(\"\\\\label{tab:results} \\\\\\\\ \\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\"Dataset & \" + \" & \".join(map(str, methods)) + \" \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\"\\\\endfirsthead\\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\"Dataset & \" + \" & \".join(map(str, methods)) + \" \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\"\\\\endhead\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\n",
    "            \"\\\\multicolumn{\"\n",
    "            + str(len(methods) + 1)\n",
    "            + \"}{r}{Continued on next page} \\\\\\\\\\n\"\n",
    "        )\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\"\\\\endfoot\\n\")\n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\endlastfoot\\n\")\n",
    "\n",
    "        for task_id in task_ids:\n",
    "            dataset_name = repo.tid_to_dataset(\n",
    "                task_id\n",
    "            )  # Convert task_id to dataset name\n",
    "            truncated_name = (\n",
    "                (dataset_name[:max_char] + \"...\")\n",
    "                if len(dataset_name) > max_char\n",
    "                else dataset_name\n",
    "            )\n",
    "            escaped_name = truncated_name.replace(\"_\", \"\\\\_\")  # Escape underscores\n",
    "            line = [str(escaped_name)]  # Ensure the first item is a string\n",
    "            method_scores = []\n",
    "\n",
    "            for method in methods:\n",
    "                method_data = df[\n",
    "                    (df[\"task_id\"] == task_id) & (df[\"method_name\"] == method)\n",
    "                ]\n",
    "                if not method_data.empty:\n",
    "                    mean_score = method_data[\"roc_auc_test\"].mean()\n",
    "                    std_dev = method_data[\"roc_auc_test\"].std()\n",
    "                    score_str = f\"{mean_score:.4f}($\\\\pm${std_dev:.4f})\"\n",
    "                    method_scores.append((mean_score, score_str))\n",
    "                else:\n",
    "                    method_scores.append((None, \"-\"))\n",
    "\n",
    "            # Determine the best score\n",
    "            best_score = max(\n",
    "                score[0] for score in method_scores if score[0] is not None\n",
    "            )\n",
    "\n",
    "            for mean_score, score_str in method_scores:\n",
    "                if mean_score is not None and np.isclose(mean_score, best_score):\n",
    "                    line.append(f\"\\\\textbf{{{score_str}}}\")\n",
    "                else:\n",
    "                    line.append(score_str)\n",
    "\n",
    "            f.write(\" & \".join(line) + \" \\\\\\\\\\n\")\n",
    "\n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\end{longtable}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../tables\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "repo = load_repository(\"D244_F3_C1530_100\", cache=True)\n",
    "create_latex_table(df, repo, filename=\"../tables/table.tex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HA-ES",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
