{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HA-ES Plotting\n",
    "\n",
    "- udpated version of plotting script used for paper \"...\"\n",
    "\n",
    "## General\n",
    "\n",
    "- imports\n",
    "- defintions\n",
    "- loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygmo as pg\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "from autorank._util import get_sorted_rank_groups\n",
    "from autorank import autorank, plot_stats\n",
    "from tabrepo import load_repository\n",
    "\n",
    "method_id_name_dict = {\n",
    "    \"GES\": \"GES\",\n",
    "    \"SINGLE_BEST\": \"Single-Best\",\n",
    "    \"QO\": \"QO-ES\",\n",
    "    \"QDO\": \"QDO-ES\",\n",
    "    \"ENS_SIZE_QDO\": \"Size-QDO-ES\",\n",
    "    \"INFER_TIME_QDO\": \"Infer-QDO-ES\",\n",
    "    \"MEMORY_QDO\": \"Memory-QDO-ES\",\n",
    "    \"DISK_QDO\": \"Diskspace-QDO-ES\",\n",
    "}\n",
    "infer_time_weights = np.linspace(0, 1, num=20)[1:]\n",
    "infer_time_weights = np.round(infer_time_weights, 2)\n",
    "multi_ges_method_ids = [f\"MULTI_GES-{time_weight:.2f}\" for time_weight in infer_time_weights]\n",
    "multi_ges_method_names = [f\"Multi-GES({time_weight:.2f})\" for time_weight in infer_time_weights]\n",
    "for id, name in zip(multi_ges_method_ids, multi_ges_method_names):\n",
    "    method_id_name_dict[id] = name\n",
    "\n",
    "print(\"Loading data. This might take a while...\")\n",
    "df = pd.read_json(\"../data/full.json\")\n",
    "\n",
    "# Map method IDs to names\n",
    "if \"method\" in df.columns:\n",
    "    df[\"method_name\"] = df[\"method\"].map(method_id_name_dict)\n",
    "else:\n",
    "    raise ValueError(\"Column 'method' not found in DataFrame\")\n",
    "df = df.dropna(subset=['method_name'])\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df[\"method_name\"].unique())\n",
    "print(df[\"method\"].unique())\n",
    "\n",
    "df[\"models_used_length\"] = df[\"models_used\"].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "### Boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_numeric_part(method_name):\n",
    "    \"\"\"\n",
    "    Extracts the numeric part from a method name string. If no numeric part is found, returns None.\n",
    "    \"\"\"\n",
    "    if isinstance(method_name, str):\n",
    "        match = re.search(r\"\\((\\d*\\.?\\d+)\\)\", method_name)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "    return None\n",
    "\n",
    "def boxplot(\n",
    "    df: pd.DataFrame,\n",
    "    y_str: str,\n",
    "    log_y_scale: bool = False,\n",
    "    log_x_scale: bool = False,\n",
    "    flip_y_axis: bool = False,\n",
    "    orient: str = \"v\",\n",
    "    rotation_x_ticks: int = 45,\n",
    "    outliers=False,\n",
    "):\n",
    "    if y_str not in df.columns:\n",
    "        raise ValueError(f\"Column '{y_str}' not found in DataFrame\")\n",
    "    \n",
    "    # Ensure all values in 'method_name' are strings\n",
    "    df[\"method_name\"] = df[\"method_name\"].astype(str)\n",
    "\n",
    "    # Extract alphabetic part and numeric part from 'method_name' for sorting\n",
    "    df[\"alphabetic\"] = df[\"method_name\"].apply(lambda x: re.split(r\"\\(\\d*\\.?\\d+\\)\", x)[0])\n",
    "    df[\"numeric\"] = df[\"method_name\"].apply(extract_numeric_part)\n",
    "    \n",
    "    # Sort by alphabetic part first, then numeric part (if any)\n",
    "    df = df.sort_values(by=[\"alphabetic\", \"numeric\"], ascending=[True, True])\n",
    "\n",
    "    # Drop the temporary columns after sorting\n",
    "    df = df.drop(columns=[\"alphabetic\", \"numeric\"])\n",
    "\n",
    "    # Plot ROC AUC scores for each method\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if orient == \"v\":\n",
    "        sns.boxplot(\n",
    "            data=df,\n",
    "            x=\"method_name\",  # or y='method_name' based on 'orient'\n",
    "            y=y_str,\n",
    "            hue=\"method_name\",  # Assigning the variable to hue\n",
    "            palette=\"pastel\",\n",
    "            linewidth=2,\n",
    "            orient=orient,\n",
    "            legend=False,  # Set legend to False if you don't need it\n",
    "            showfliers=outliers,\n",
    "        )\n",
    "    elif orient == \"h\":\n",
    "        sns.boxplot(\n",
    "            data=df,\n",
    "            x=y_str,\n",
    "            y=\"method_name\",  # or y='method_name' based on 'orient'\n",
    "            hue=\"method_name\",  # Assigning the variable to hue\n",
    "            palette=\"pastel\",\n",
    "            linewidth=2,\n",
    "            orient=orient,\n",
    "            legend=False,  # Set legend to False if you don't need it\n",
    "            showfliers=outliers,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Orient '{orient}' not supported\")\n",
    "\n",
    "    if orient == \"v\":\n",
    "        plt.ylabel(y_str)\n",
    "        plt.xlabel(\"Ensemble Method\")\n",
    "    elif orient == \"h\":\n",
    "        plt.xlabel(y_str)\n",
    "        plt.ylabel(\"Ensemble Method\")\n",
    "\n",
    "    if log_y_scale:\n",
    "        plt.yscale(\"log\")\n",
    "    if log_x_scale:\n",
    "        plt.xscale(\"log\")\n",
    "    if flip_y_axis:\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.xticks(rotation=rotation_x_ticks)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Check if the directory exists, if not, create it\n",
    "    directory = \"../plots\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Save to file\n",
    "    plt.savefig(f\"{directory}/boxplot_{y_str}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(f\"{directory}/boxplot_{y_str}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "def interactive_boxplot(df, column_options):\n",
    "    y_str_widget = widgets.Dropdown(\n",
    "        options=column_options,\n",
    "        description=\"Y-axis column:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    log_y_scale_widget = widgets.Checkbox(value=False, description=\"Log-scale Y-axis\")\n",
    "    log_x_scale_widget = widgets.Checkbox(value=False, description=\"Log-scale X-axis\")\n",
    "    flip_y_axis_widget = widgets.Checkbox(value=False, description=\"Flip Y-axis\")\n",
    "    orient_widget = widgets.RadioButtons(\n",
    "        options=[\"v\", \"h\"],\n",
    "        value=\"h\",\n",
    "        description=\"Orientation:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    rotation_x_ticks_widget = widgets.IntSlider(\n",
    "        value=0, min=0, max=90, step=5, description=\"Rotation X-ticks\"\n",
    "    )\n",
    "\n",
    "    ui = widgets.VBox(\n",
    "        [\n",
    "            y_str_widget,\n",
    "            log_y_scale_widget,\n",
    "            log_x_scale_widget,\n",
    "            flip_y_axis_widget,\n",
    "            orient_widget,\n",
    "            rotation_x_ticks_widget,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    out = widgets.interactive_output(\n",
    "        boxplot,\n",
    "        {\n",
    "            \"df\": widgets.fixed(df),\n",
    "            \"y_str\": y_str_widget,\n",
    "            \"log_y_scale\": log_y_scale_widget,\n",
    "            \"log_x_scale\": log_x_scale_widget,\n",
    "            \"flip_y_axis\": flip_y_axis_widget,\n",
    "            \"orient\": orient_widget,\n",
    "            \"rotation_x_ticks\": rotation_x_ticks_widget,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical Difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd_evaluation(\n",
    "    hypervolumes,\n",
    "    maximize_metric=True,\n",
    "    plt_title=\"Critical Difference Plot\",\n",
    "    filename=\"CriticalDifferencePlot.png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    hypervolumes: DataFrame with method names as columns and tasks as rows, each cell contains hypervolume.\n",
    "    maximize_metric: Boolean, True if higher values are better.\n",
    "    output_path: Where to save the plot, if None, plot will not be saved.\n",
    "    plt_title: Title of the plot.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    rank_data = -hypervolumes if maximize_metric else hypervolumes\n",
    "\n",
    "    # Run autorank\n",
    "    result = autorank(rank_data, alpha=0.05, verbose=False, order=\"ascending\")\n",
    "\n",
    "    # Plot with updated font size\n",
    "    plt.close(\"all\")\n",
    "    width = 6\n",
    "    fig, ax = plt.subplots(figsize=(12, width))\n",
    "    plt.rcParams.update({\"font.size\": 20})\n",
    "\n",
    "    plot_stats(result, ax=ax)\n",
    "    ax.tick_params(axis=\"both\", labelsize=20)  # Set font size for axis ticks\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    ax.set_xticklabels(labels, fontsize=20)  # Adjust fontsize as needed\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(filename, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_options = [\"models_used_length\", \"normalized_roc_auc_test\", \"normalized_roc_auc_val\", \"inference_time\"]\n",
    "\n",
    "#filtered_df = df[~df['method_name'].isin(['Multi-GES(0.43)', 'Multi-GES(0.50)'])].copy()\n",
    "interactive_boxplot(df, column_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['models_used_tuple'] = df['models_used'].apply(tuple)\n",
    "\n",
    "# Group the DataFrame by 'method_name' and compute total and unique counts\n",
    "result_df = df.groupby('method_name').agg(\n",
    "    total_solutions=('models_used', 'count'),\n",
    "    unique_solutions=('models_used_tuple', 'nunique')\n",
    ")\n",
    "\n",
    "# Calculate the percentage of unique solutions\n",
    "result_df['percentage_unique_solutions'] = (\n",
    "    result_df['unique_solutions'] / result_df['total_solutions'] * 100\n",
    ")\n",
    "\n",
    "# Optionally reset the index if you want 'method_name' as a column\n",
    "result_df = result_df.reset_index()\n",
    "\n",
    "# Plot the bar chart for total solutions\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = result_df.plot(kind='bar', x='method_name', y='total_solutions', color='lightblue', legend=False)\n",
    "\n",
    "# Plot the percentage of unique solutions as a line graph\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(result_df['method_name'], result_df['percentage_unique_solutions'], color='red', marker='o', label='Percentage Unique Solutions')\n",
    "ax2.set_ylabel('Percentage of Unique Solutions (%)')\n",
    "\n",
    "# Set titles and labels\n",
    "# ax.set_title(\"Total Solutions and Percentage of Unique Solutions per Method\")\n",
    "ax.set_ylabel(\"Total Solutions\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto Front Eval\n",
    "\n",
    "### True Pareto Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParetoFront(df, objectives, return_mask=False):\n",
    "    \"\"\"\n",
    "    Identify the Pareto-efficient points in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - objectives: list of objective column names.\n",
    "    - return_mask: if True, return a boolean mask; else return indices.\n",
    "\n",
    "    Returns:\n",
    "    - Boolean mask or indices of Pareto-efficient points.\n",
    "    \"\"\"\n",
    "    data = df[objectives].values\n",
    "    is_efficient = np.ones(data.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(data):\n",
    "        if is_efficient[i]:\n",
    "            is_efficient[is_efficient] = np.any(data[is_efficient] < c, axis=1)  # Keep any point with a lower value\n",
    "            is_efficient[i] = True  # And keep self\n",
    "    if return_mask:\n",
    "        return is_efficient\n",
    "    else:\n",
    "        return np.where(is_efficient)[0]\n",
    "    \n",
    "def calculate_pareto_fronts(df, objectives):\n",
    "    \"\"\"\n",
    "    Calculate the Pareto front points for all solutions in the DataFrame per task_id, seed, and fold.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - objectives: list of two objective column names.\n",
    "\n",
    "    Returns:\n",
    "    - A nested dictionary with structure {task_id: {seed: {fold: pareto_front_df}}}\n",
    "    \"\"\"\n",
    "    if len(objectives) != 2:\n",
    "        raise ValueError(\"Exactly two objectives must be provided.\")\n",
    "\n",
    "    # Initialize a dictionary to store the Pareto fronts\n",
    "    pareto_fronts = {}\n",
    "\n",
    "    # Iterate over unique task_ids\n",
    "    for task_id in df[\"task_id\"].unique():\n",
    "        pareto_fronts[task_id] = {}\n",
    "\n",
    "        # Iterate over unique seeds\n",
    "        for seed in df[\"seed\"].unique():\n",
    "            pareto_fronts[task_id][seed] = {}\n",
    "\n",
    "            # Iterate over unique folds\n",
    "            for fold in df[\"fold\"].unique():\n",
    "                # Filter the DataFrame for the current task_id, seed, and fold\n",
    "                df_fold = df[\n",
    "                    (df[\"task_id\"] == task_id)\n",
    "                    & (df[\"seed\"] == seed)\n",
    "                    & (df[\"fold\"] == fold)\n",
    "                ]\n",
    "\n",
    "                if df_fold.empty:\n",
    "                    continue  # Skip if no data for this combination\n",
    "\n",
    "                # Compute Pareto front for df_fold\n",
    "                is_efficient = getParetoFront(df_fold, objectives, return_mask=True)\n",
    "                pareto_front_df = df_fold[is_efficient]\n",
    "\n",
    "                # Store the Pareto front DataFrame\n",
    "                pareto_fronts[task_id][seed][fold] = pareto_front_df\n",
    "\n",
    "    return pareto_fronts\n",
    "\n",
    "\n",
    "\n",
    "def plot_pareto_fronts(pareto_fronts, df, objectives, seed_to_plot=0):\n",
    "    \"\"\"\n",
    "    Plot Pareto fronts with outlined and semi-transparent points, only for the specified seed.\n",
    "\n",
    "    Parameters:\n",
    "    - pareto_fronts: nested dictionary with structure {task_id: {seed: {fold: pareto_front_df}}}\n",
    "    - df: original DataFrame containing all points.\n",
    "    - objectives: list of two objective column names.\n",
    "    - seed_to_plot: the seed for which to plot Pareto fronts.\n",
    "    \"\"\"\n",
    "    # Unpack objective names for labeling\n",
    "    obj_x, obj_y = objectives\n",
    "\n",
    "    point_size = 100  # Adjust point size as needed\n",
    "\n",
    "    # Iterate over task_id and fold combinations for the specified seed\n",
    "    for task_id, seeds in pareto_fronts.items():\n",
    "        if seed_to_plot in seeds:\n",
    "            for fold, pareto_front_df in seeds[seed_to_plot].items():\n",
    "                # Filter out non-Pareto points\n",
    "                df_fold = df[(df[\"task_id\"] == task_id) & (df[\"seed\"] == seed_to_plot) & (df[\"fold\"] == fold)]\n",
    "                non_pareto_df = df_fold[~df_fold.index.isin(pareto_front_df.index)]\n",
    "\n",
    "                # Plot each Pareto front using Seaborn\n",
    "                plt.figure(figsize=(10, 8))\n",
    "\n",
    "                # Plot non-Pareto points in skyblue with a dark outline and semi-transparency\n",
    "                sns.scatterplot(\n",
    "                    x=non_pareto_df[obj_x],\n",
    "                    y=non_pareto_df[obj_y],\n",
    "                    color='skyblue',\n",
    "                    edgecolor='black',\n",
    "                    marker='o',\n",
    "                    s=point_size,\n",
    "                    alpha=0.6,\n",
    "                    label='Non-Pareto points'\n",
    "                )\n",
    "\n",
    "                # Plot Pareto front points in darkred with a black outline and semi-transparency\n",
    "                sns.scatterplot(\n",
    "                    x=pareto_front_df[obj_x],\n",
    "                    y=pareto_front_df[obj_y],\n",
    "                    color='red',\n",
    "                    edgecolor='black',\n",
    "                    marker='o',\n",
    "                    s=point_size,\n",
    "                    alpha=0.8,\n",
    "                    label='Pareto front points'\n",
    "                )\n",
    "\n",
    "                # Customize axis labels, title, and legend for readability\n",
    "                plt.xlabel(obj_x, fontsize=14)\n",
    "                plt.ylabel(obj_y, fontsize=14)\n",
    "                plt.title(f'Pareto Front for Task {task_id}, Seed {seed_to_plot}, Fold {fold}', fontsize=16)\n",
    "                plt.legend(fontsize=12)\n",
    "                plt.grid(True)\n",
    "\n",
    "                # Set axis limits to [0, 1]\n",
    "                plt.xlim(0, 1)\n",
    "                plt.ylim(0, 1)\n",
    "\n",
    "                # Create directory if it doesn't exist and save the plot\n",
    "                directory = f\"../plots/pareto_fronts/{seed_to_plot}\"\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                plt.savefig(f\"{directory}/{task_id}_{fold}.png\")\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your objectives\n",
    "objectives = ['normalized_roc_auc', 'normalized_time']\n",
    "\n",
    "# Calculate Pareto fronts\n",
    "# pareto_fronts = calculate_pareto_fronts(df, objectives)\n",
    "\n",
    "# plot_pareto_fronts(pareto_fronts, df, objectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypervolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pareto_efficient(costs, return_mask=True):\n",
    "    is_efficient = np.ones(costs.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(costs):\n",
    "        is_efficient[i] = not np.any(\n",
    "            np.all(costs <= c, axis=1) & np.any(costs < c, axis=1)\n",
    "        )\n",
    "    return is_efficient if return_mask else costs[is_efficient]\n",
    "\n",
    "\n",
    "def calculate_average_hypervolumes(df, method_name, objectives: list = [\"normalized_roc_auc\", \"normalized_time\"]):\n",
    "    if len(objectives) != 2: \n",
    "        return\n",
    "    df_method = df[df[\"method_name\"] == method_name]\n",
    "    hypervolumes = {}\n",
    "\n",
    "    # Iterate over unique task_ids\n",
    "    for task_id in df_method[\"task_id\"].unique():\n",
    "        seed_hypervolumes = []  # Store hypervolumes for each seed\n",
    "\n",
    "        for seed in df_method[\"seed\"].unique():\n",
    "            fold_hypervolumes = []  # Store hypervolumes for each fold under the current seed\n",
    "\n",
    "            for fold in df_method[\"fold\"].unique():\n",
    "                df_fold = df_method[\n",
    "                    (df_method[\"task_id\"] == task_id)\n",
    "                    & (df_method[\"seed\"] == seed)\n",
    "                    & (df_method[\"fold\"] == fold)\n",
    "                ]\n",
    "\n",
    "                # Use a different variable name here\n",
    "                objective_values = np.array(\n",
    "                    [\n",
    "                        df_fold[objectives[0]].values,\n",
    "                        df_fold[objectives[1]].values,\n",
    "                    ]\n",
    "                ).T\n",
    "                is_efficient = is_pareto_efficient(objective_values)\n",
    "                efficient_objectives = objective_values[is_efficient]\n",
    "\n",
    "                ref_point = [\n",
    "                    1.01,\n",
    "                    1.01,\n",
    "                ]  # Reference point beyond the worst values of objectives\n",
    "                hv = pg.hypervolume(efficient_objectives)\n",
    "                hypervolume = hv.compute(ref_point)\n",
    "                fold_hypervolumes.append(hypervolume)\n",
    "\n",
    "            # Average hypervolumes across all folds for a given seed\n",
    "            if fold_hypervolumes:\n",
    "                average_fold_hypervolume = np.mean(fold_hypervolumes)\n",
    "                seed_hypervolumes.append(average_fold_hypervolume)\n",
    "\n",
    "        # Average the averaged fold hypervolumes across seeds\n",
    "        if seed_hypervolumes:\n",
    "            average_seed_hypervolume = np.mean(seed_hypervolumes)\n",
    "            hypervolumes[task_id] = average_seed_hypervolume\n",
    "\n",
    "    return hypervolumes\n",
    "\n",
    "\n",
    "\n",
    "def find_non_dominated(points):\n",
    "    \"\"\"Identify the indices of non-dominated points.\"\"\"\n",
    "    is_efficient = np.ones(points.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(points):\n",
    "        is_efficient[i] = not np.any(\n",
    "            np.all(points <= c, axis=1) & np.any(points < c, axis=1)\n",
    "        )\n",
    "    return np.where(is_efficient)[0]\n",
    "\n",
    "\n",
    "def plot_hypervolumes(all_hypervolumes, directory: str = \"../plots/\"):\n",
    "    # Prepare the data for plotting\n",
    "    methods = list(all_hypervolumes.keys())  # Method names\n",
    "    hv_values = [list(all_hypervolumes[method].values()) for method in methods]\n",
    "    data = []\n",
    "\n",
    "    # Creating a DataFrame suitable for Seaborn\n",
    "    for method_index, values in enumerate(hv_values):\n",
    "        for value in values:\n",
    "            data.append({\"Method\": methods[method_index], \"Hypervolume\": value})\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Calculate medians for each method and sort by median\n",
    "    median_order = df.groupby(\"Method\")[\"Hypervolume\"].median().sort_values(ascending=False).index\n",
    "\n",
    "    # Set the figure size and style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Use seaborn's boxplot to plot the DataFrame, ordered by median values\n",
    "    ax = sns.boxplot(\n",
    "        y=\"Method\", x=\"Hypervolume\", data=df, hue=\"Method\", palette=\"Set2\",\n",
    "        orient=\"h\", order=median_order\n",
    "    )\n",
    "\n",
    "    # Set titles and labels\n",
    "    ax.set_ylabel(\"Method\", fontsize=20)\n",
    "    ax.set_xlabel(\"Hypervolume\", fontsize=20)\n",
    "\n",
    "    # Set font size for ticks\n",
    "    ax.tick_params(axis=\"x\", labelrotation=45, labelsize=16)\n",
    "    ax.tick_params(axis=\"y\", labelsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(directory + \"hypervolume_comparison.pdf\", dpi=300)\n",
    "    plt.savefig(directory + \"hypervolume_comparison.png\", dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = df[\"method_name\"].unique() #[\n",
    "    # \"GES\",\n",
    "    # \"Multi-GES(0.00)\"\n",
    "    # \"Single-Best\",\n",
    "    # \"QDO-ES\",\n",
    "    # \"Infer-QDO-ES\",\n",
    "    # \"Size-QDO-ES\",\n",
    "    # \"Memory-QDO-ES\",\n",
    "    # \"Diskspace-QDO-ES\",\n",
    "# ]\n",
    "all_hypervolumes = {}\n",
    "\n",
    "for method in methods:\n",
    "    all_hypervolumes[method] = calculate_average_hypervolumes(df, method, [\"normalized_roc_auc\", \"normalized_time\"])    \n",
    "\n",
    "plot_hypervolumes(all_hypervolumes)\n",
    "hypervolumes_df = pd.DataFrame(all_hypervolumes)\n",
    "hypervolumes_df.to_csv(\"../data/hypervolumes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for method, tasks in all_hypervolumes.items():\n",
    "    for task_id, hypervolume in tasks.items():\n",
    "        data.append({\"Task\": task_id, \"Method\": method, \"Hypervolume\": hypervolume})\n",
    "\n",
    "df_hypervolumes = pd.DataFrame(data)\n",
    "pivot_hypervolumes = df_hypervolumes.pivot(\n",
    "    index=\"Task\", columns=\"Method\", values=\"Hypervolume\"\n",
    ")\n",
    "\n",
    "# Now you can use the modified cd_evaluation function\n",
    "hv_result = cd_evaluation(\n",
    "    pivot_hypervolumes,\n",
    "    maximize_metric=True,\n",
    "    plt_title=\"Hypervolume Critical Difference Plot\",\n",
    "    filename=\"../plots/CDPHypervolumes.pdf\",\n",
    ")\n",
    "hv_result = cd_evaluation(\n",
    "    pivot_hypervolumes,\n",
    "    maximize_metric=True,\n",
    "    plt_title=\"Hypervolume Critical Difference Plot\",\n",
    "    filename=\"../plots/CDPHypervolumes.png\",\n",
    ")\n",
    "print(hv_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Generational Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.indicators.igd import IGD\n",
    "\n",
    "def calculate_IGD_per_method(df, objectives):\n",
    "    \"\"\"\n",
    "    Calculate the IGD for each method_name per task_id, seed, and fold.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - objectives: list of two objective column names.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with columns [task_id, seed, fold, method_name, IGD].\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Iterate over unique task_ids\n",
    "    for task_id in df[\"task_id\"].unique():\n",
    "        # Iterate over unique seeds\n",
    "        for seed in df[\"seed\"].unique():\n",
    "            # Iterate over unique folds\n",
    "            for fold in df[\"fold\"].unique():\n",
    "                # Filter the DataFrame for the current task_id, seed, and fold\n",
    "                df_fold = df[\n",
    "                    (df[\"task_id\"] == task_id)\n",
    "                    & (df[\"seed\"] == seed)\n",
    "                    & (df[\"fold\"] == fold)\n",
    "                ]\n",
    "\n",
    "                if df_fold.empty:\n",
    "                    continue  # Skip if no data for this combination\n",
    "\n",
    "                # Compute the reference Pareto front (from all methods)\n",
    "                is_efficient = getParetoFront(df_fold, objectives, return_mask=True)\n",
    "                reference_pareto_front = df_fold[is_efficient]\n",
    "                pf_points = reference_pareto_front[objectives].values\n",
    "\n",
    "                # Iterate over unique method_names\n",
    "                for method_name in df_fold[\"method_name\"].unique():\n",
    "                    df_method = df_fold[df_fold[\"method_name\"] == method_name]\n",
    "\n",
    "                    # Compute Pareto front for df_method\n",
    "                    is_efficient = getParetoFront(df_method, objectives, return_mask=True)\n",
    "                    method_pareto_front = df_method[is_efficient]\n",
    "                    method_points = method_pareto_front[objectives].values\n",
    "\n",
    "                    if len(method_points) == 0 or len(pf_points) == 0:\n",
    "                        igd_value = np.nan\n",
    "                    else:\n",
    "                        # Calculate IGD\n",
    "                        ind = IGD(pf_points)\n",
    "                        igd_value = ind(method_points)\n",
    "\n",
    "                    # Append results\n",
    "                    results.append({\n",
    "                        \"task_id\": task_id,\n",
    "                        \"seed\": seed,\n",
    "                        \"fold\": fold,\n",
    "                        \"method_name\": method_name,\n",
    "                        \"IGD\": igd_value\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def plot_IGD_values_with_medians(igd_results):\n",
    "    \"\"\"\n",
    "    Plot the IGD values for each method, sorted by median IGD, and annotate the median values on the plot.\n",
    "\n",
    "    Parameters:\n",
    "    - igd_results: DataFrame with columns [task_id, seed, fold, method_name, IGD].\n",
    "    \"\"\"\n",
    "    # Remove any NaN IGD values\n",
    "    igd_results_clean = igd_results.dropna(subset=['IGD'])\n",
    "\n",
    "    # Compute median IGD per method and sort methods\n",
    "    median_igd = igd_results_clean.groupby('method_name')['IGD'].median().sort_values()\n",
    "    sorted_methods = median_igd.index.tolist()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.boxplot(data=igd_results_clean, x='method_name', y='IGD', order=sorted_methods)\n",
    "    \n",
    "    # Annotate median values on top of the box plots\n",
    "    for method in sorted_methods:\n",
    "        median_val = median_igd[method]\n",
    "        x = sorted_methods.index(method)  # Find the position on the x-axis\n",
    "        y = median_val\n",
    "        ax.text(x, y + 0.01, f'{median_val:.2f}', ha='center', va='bottom', color='black', fontweight='bold')\n",
    "\n",
    "    plt.title('IGD Values per Method (Sorted by Median IGD)')\n",
    "    plt.ylabel('IGD')\n",
    "    plt.xlabel('Method Name')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IGD per method\n",
    "igd_results = calculate_IGD_per_method(df, [\"negated_normalized_roc_auc\", \"normalized_time\"])\n",
    "\n",
    "# Plot IGD values per method, sorted by median IGD, and annotate median values\n",
    "plot_IGD_values_with_medians(igd_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'method_name' and 'task_id', and aggregate with 'sum'\n",
    "agg_igd = igd_results.groupby([\"method_name\", \"task_id\"]).agg('sum').reset_index()\n",
    "\n",
    "# Pivot the data to have 'task_id' as the index and 'method_name' as columns\n",
    "igd_pivots = agg_igd.pivot(\n",
    "    index=\"task_id\", columns=\"method_name\", values=\"IGD\"\n",
    ")\n",
    "\n",
    "# Perform the critical difference evaluation\n",
    "igd_result = cd_evaluation(\n",
    "    igd_pivots,\n",
    "    maximize_metric=False,\n",
    "    plt_title=\"Hypervolume Critical Difference Plot\",\n",
    "    filename=\"../plots/CDP_IGD.png\",\n",
    ")\n",
    "\n",
    "# Repeat the same evaluation if necessary\n",
    "igd_result = cd_evaluation(\n",
    "    igd_pivots,\n",
    "    maximize_metric=False,\n",
    "    plt_title=\"Hypervolume Critical Difference Plot\",\n",
    "    filename=\"../plots/CDP_IGD.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "def calculate_spacing(pareto_front_df, objectives):\n",
    "    points = pareto_front_df[objectives].values\n",
    "    n_points = len(points)\n",
    "    if n_points <= 2:\n",
    "        return np.nan  \n",
    "    dist_matrix = distance_matrix(points, points)\n",
    "    np.fill_diagonal(dist_matrix, np.inf)\n",
    "    min_distances = dist_matrix.min(axis=1)\n",
    "    if np.any(min_distances == np.inf):\n",
    "        return np.nan\n",
    "    mean_distance = np.mean(min_distances)\n",
    "    variance = np.sum((mean_distance - min_distances)**2) / (n_points - 1)\n",
    "    return variance\n",
    "\n",
    "\n",
    "def calculate_spacing_per_method(df, objectives):\n",
    "    \"\"\"\n",
    "    Calculate the spacing for each method_name per task_id, seed, and fold.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - objectives: list of two objective column names.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with columns [task_id, seed, fold, method_name, spacing].\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Iterate over unique task_ids\n",
    "    for task_id in df[\"task_id\"].unique():\n",
    "        # Iterate over unique seeds\n",
    "        for seed in df[\"seed\"].unique():\n",
    "            # Iterate over unique folds\n",
    "            for fold in df[\"fold\"].unique():\n",
    "                # Filter the DataFrame for the current task_id, seed, and fold\n",
    "                df_fold = df[\n",
    "                    (df[\"task_id\"] == task_id)\n",
    "                    & (df[\"seed\"] == seed)\n",
    "                    & (df[\"fold\"] == fold)\n",
    "                ]\n",
    "\n",
    "                if df_fold.empty:\n",
    "                    continue  # Skip if no data for this combination\n",
    "\n",
    "                # Iterate over unique method_names\n",
    "                for method_name in df_fold[\"method_name\"].unique():\n",
    "                    df_method = df_fold[df_fold[\"method_name\"] == method_name]\n",
    "\n",
    "                    # Compute Pareto front for df_method\n",
    "                    is_efficient = getParetoFront(df_method, objectives, return_mask=True)\n",
    "                    pareto_front_df = df_method[is_efficient]\n",
    "\n",
    "                    # Calculate spacing\n",
    "                    spacing = calculate_spacing(pareto_front_df, objectives)\n",
    "\n",
    "                    # Append results\n",
    "                    results.append({\n",
    "                        \"task_id\": task_id,\n",
    "                        \"seed\": seed,\n",
    "                        \"fold\": fold,\n",
    "                        \"method_name\": method_name,\n",
    "                        \"spacing\": spacing\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def plot_spacing_metrics(spacing_metrics_df):\n",
    "    \"\"\"\n",
    "    Plot the spacing metrics using a bar plot, sorted by median spacing values.\n",
    "\n",
    "    Parameters:\n",
    "    - spacing_metrics_df: DataFrame with columns ['method_name', 'spacing']\n",
    "    \"\"\"\n",
    "    # Calculate the median spacing for each method and sort by these values\n",
    "    median_order = spacing_metrics_df.groupby(\"method_name\")[\"spacing\"].median().sort_values(ascending=False).index\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='method_name', y='spacing', data=spacing_metrics_df, showfliers=False, order=median_order)\n",
    "    plt.title('Spacing per Method')\n",
    "    plt.xlabel('Method Name')\n",
    "    plt.ylabel('Spacing Metric')\n",
    "    #plt.yscale('log')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the spacing results\n",
    "spacing_results = calculate_spacing_per_method(df, [\"negated_normalized_roc_auc\", \"normalized_time\"])\n",
    "spacing_results = spacing_results.dropna(subset=['spacing'])\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "plot_spacing_metrics(spacing_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame 'spacing_results' similar to 'igd_results' that contains the 'spacing' values\n",
    "# and the relevant columns: 'task_id', 'method_name', 'spacing'\n",
    "\n",
    "# Group by 'method_name' and 'task_id', and aggregate by summing the spacing values\n",
    "agg_spacing = spacing_results.groupby([\"method_name\", \"task_id\"]).agg('sum').reset_index()\n",
    "\n",
    "# Pivot the data to have 'task_id' as the index and 'method_name' as columns for spacing values\n",
    "spacing_pivots = agg_spacing.pivot(\n",
    "    index=\"task_id\", columns=\"method_name\", values=\"spacing\"\n",
    ")\n",
    "\n",
    "# Create a CDP plot for the spacing values\n",
    "spacing_result = cd_evaluation(\n",
    "    spacing_pivots,\n",
    "    maximize_metric=False,\n",
    "    plt_title=\"Spacing Critical Difference Plot\",\n",
    "    filename=\"../plots/CDP_Spacing.png\",\n",
    ")\n",
    "\n",
    "# Optionally repeat the evaluation if necessary\n",
    "spacing_result = cd_evaluation(\n",
    "    spacing_pivots,\n",
    "    maximize_metric=False,\n",
    "    plt_title=\"Spacing Critical Difference Plot\",\n",
    "    filename=\"../plots/CDP_Spacing.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It Together\n",
    "\n",
    "\n",
    "#### Radar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "import re\n",
    "\n",
    "# Extracting relevant values (meanrank) from each dataframe\n",
    "metrics = ['Inverted Gen Dist', 'Spacing', 'Hypervolume']\n",
    "\n",
    "# Sort each dataframe by the index (method) to ensure alignment\n",
    "igd_sorted = igd_result.rankdf.sort_index()\n",
    "spacing_sorted = spacing_result.rankdf.sort_index()\n",
    "hv_sorted = hv_result.rankdf.sort_index()\n",
    "\n",
    "# Combine the meanrank values, now correctly aligned by method\n",
    "combined_df = pd.DataFrame({\n",
    "    'Method': hv_sorted.index,\n",
    "    'IGD': igd_sorted['meanrank'],\n",
    "    'Spacing': spacing_sorted['meanrank'],\n",
    "    'Hypervolume': hv_sorted['meanrank']\n",
    "})\n",
    "\n",
    "# Set 'Method' as the index\n",
    "combined_df.set_index('Method', inplace=True)\n",
    "\n",
    "# Normalize each metric independently\n",
    "for metric in ['IGD', 'Spacing', 'Hypervolume']:\n",
    "    combined_df[metric] = -(combined_df[metric] - combined_df[metric].min()) / (combined_df[metric].max() - combined_df[metric].min())\n",
    "combined_df.loc['Multi-GES(1.00)', 'Spacing'] = -1\n",
    "combined_df.loc['Single-Best', 'Spacing'] = -1\n",
    "\n",
    "# Display the final normalized combined_df to verify correctness\n",
    "print(combined_df)\n",
    "\n",
    "# Function to extract the time weight from the method name\n",
    "def extract_time_weight(method_name):\n",
    "    if method_name == \"GES\":\n",
    "        return 0.0\n",
    "    match = re.search(r\"\\(([\\d.]+)\\)\", method_name)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    return 0.0\n",
    "\n",
    "# Create a time weight list based on method names\n",
    "time_weights = [extract_time_weight(method) for method in combined_df.index]\n",
    "\n",
    "# Sort the dataframe and the time weights based on the time weights\n",
    "sorted_indices = np.argsort(time_weights)\n",
    "combined_df = combined_df.iloc[sorted_indices]\n",
    "time_weights = np.array(time_weights)[sorted_indices]\n",
    "\n",
    "# Normalize the time weights for color mapping\n",
    "norm_time_weights = (np.array(time_weights) - min(time_weights)) / (max(time_weights) - min(time_weights))\n",
    "\n",
    "# Generate a colormap (e.g., using the 'viridis' colormap)\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "# Function to create a radar chart with color based on time weight\n",
    "def create_radar_chart(df, title, time_weights, cmap):\n",
    "    labels = df.columns\n",
    "    num_vars = len(labels)\n",
    "\n",
    "    # Compute angle of each axis\n",
    "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "\n",
    "    # Make the plot circular\n",
    "    angles += angles[:1]\n",
    "\n",
    "    # Plot for each method\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "    for i, method in enumerate(df.index):\n",
    "        values = df.loc[method].tolist()\n",
    "        values += values[:1]  # Ensure the plot closes the circle\n",
    "        \n",
    "        if method == 'Single-Best':\n",
    "            color = 'red'  # Use red color for 'Single-Best'\n",
    "        else:\n",
    "            color = cmap(norm_time_weights[i])  # Color based on time weight\n",
    "        \n",
    "        # Plot each method's radar plot with corresponding color\n",
    "        ax.fill(angles, values, label=method, color=color, alpha=0.25)\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', color=color, alpha=0.6)\n",
    "\n",
    "    # Draw one axe per variable and add labels\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    \n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.xticks(angles[:-1], labels)\n",
    "\n",
    "    # Add title\n",
    "    plt.title(title, size=20, color='black', y=1.1)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Create the radar chart with color scale based on time weight\n",
    "create_radar_chart(combined_df, \"Comparison of Methods Across Metrics (Colored by Time Weight)\", time_weights, cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Coordinates Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Add a 'Method' column to use as the class in the parallel coordinates plot\n",
    "combined_df['Method'] = combined_df.index\n",
    "\n",
    "# Separate Single-Best for distinct coloring\n",
    "single_best_df = combined_df[combined_df['Method'] == 'Single-Best']\n",
    "other_methods_df = combined_df[combined_df['Method'] != 'Single-Best']\n",
    "\n",
    "# Plot other methods with the colormap\n",
    "plt.figure(figsize=(10, 6))\n",
    "parallel_coordinates(other_methods_df, class_column='Method', colormap='viridis', alpha=0.8, linewidth=2)\n",
    "\n",
    "# Overlay Single-Best in a distinct color (red) with a thicker line\n",
    "parallel_coordinates(single_best_df, class_column='Method', color=['red'], alpha=0.9, linewidth=3)\n",
    "\n",
    "# Add plot labels and title\n",
    "plt.title('Parallel Coordinates Plot of Methods Across Metrics', fontsize=16)\n",
    "plt.ylabel('Normalized Score')\n",
    "plt.xticks(rotation=15)\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df = combined_df\n",
    "best_igd_method = best_df.loc[best_df['IGD'].idxmax(), 'Method']\n",
    "best_spacing_method = best_df.loc[best_df['Spacing'].idxmax(), 'Method']\n",
    "best_hypervolume_method = best_df.loc[best_df['Hypervolume'].idxmax(), 'Method']\n",
    "\n",
    "# Overall Best Performer (mean rank across all metrics)\n",
    "best_df['Average_Rank'] = best_df[['IGD', 'Spacing', 'Hypervolume']].mean(axis=1)\n",
    "best_overall_method = best_df.loc[best_df['Average_Rank'].idxmax(), 'Method']\n",
    "\n",
    "best_df['IGD_deviation'] = (best_df['IGD'] - best_df['IGD'].mean()).abs()\n",
    "best_df['Spacing_deviation'] = (best_df['Spacing'] - best_df['Spacing'].mean()).abs()\n",
    "best_df['Hypervolume_deviation'] = (best_df['Hypervolume'] - best_df['Hypervolume'].mean()).abs()\n",
    "\n",
    "# Summing the deviations to get a 'balance' score\n",
    "best_df['Total_deviation'] = best_df[['IGD_deviation', 'Spacing_deviation', 'Hypervolume_deviation']].sum(axis=1)\n",
    "\n",
    "# The method with the smallest total deviation is the most balanced\n",
    "balanced_method = best_df.loc[best_df['Total_deviation'].idxmin(), 'Method']\n",
    "\n",
    "# Reorganizing the final results\n",
    "best_methods = {\n",
    "    'Best Overall Performer': best_overall_method,\n",
    "    'Best for IGD': best_igd_method,\n",
    "    'Best for Spacing': best_spacing_method,\n",
    "    'Best for Hypervolume': best_hypervolume_method,\n",
    "    'Best Balanced Performer': balanced_method\n",
    "}\n",
    "\n",
    "best_methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto Front Topography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your objectives\n",
    "objectives = ['negated_normalized_roc_auc', 'normalized_time']\n",
    "\n",
    "# Get unique method names\n",
    "method_names = df[\"method_name\"].unique()\n",
    "\n",
    "# Calculate Pareto fronts for each method\n",
    "method_pareto_fronts = {}\n",
    "for method in method_names:\n",
    "    df_method = df[df[\"method_name\"] == method]\n",
    "    method_pareto_fronts[method] = calculate_pareto_fronts(df_method, objectives)\n",
    "\n",
    "# Function to count the number of solutions in Pareto fronts\n",
    "def count_pareto_solutions(pareto_fronts):\n",
    "    counts = {}\n",
    "    for task_id in pareto_fronts:\n",
    "        for seed in pareto_fronts[task_id]:\n",
    "            for fold in pareto_fronts[task_id][seed]:\n",
    "                pareto_front_df = pareto_fronts[task_id][seed][fold]\n",
    "                n_solutions = len(pareto_front_df)\n",
    "                key = (task_id, seed, fold)\n",
    "                counts[key] = n_solutions\n",
    "    return counts\n",
    "\n",
    "# Count solutions in the method-specific Pareto fronts\n",
    "method_pareto_counts = {}\n",
    "for method in method_names:\n",
    "    method_pareto_counts[method] = count_pareto_solutions(method_pareto_fronts[method])\n",
    "\n",
    "# Calculate total number of solutions per method, task_id, seed, fold\n",
    "total_counts = df.groupby(['method_name', 'task_id', 'seed', 'fold']).size().reset_index(name='total_solutions')\n",
    "\n",
    "# Prepare data for plotting counts\n",
    "data_list = []\n",
    "\n",
    "# Add the method-specific Pareto fronts and total counts\n",
    "for method in method_names:\n",
    "    counts = method_pareto_counts[method]\n",
    "    for key, n_solutions in counts.items():\n",
    "        task_id, seed, fold = key\n",
    "        # Get total number of solutions for this method, task_id, seed, fold\n",
    "        total_solutions = total_counts[\n",
    "            (total_counts['method_name'] == method) &\n",
    "            (total_counts['task_id'] == task_id) &\n",
    "            (total_counts['seed'] == seed) &\n",
    "            (total_counts['fold'] == fold)\n",
    "        ]['total_solutions'].values[0]\n",
    "        data_list.append({\n",
    "            'method_name': method,\n",
    "            'task_id': task_id,\n",
    "            'seed': seed,\n",
    "            'fold': fold,\n",
    "            'n_pareto_solutions': n_solutions,\n",
    "            'total_solutions': total_solutions\n",
    "        })\n",
    "\n",
    "pareto_counts_df = pd.DataFrame(data_list)\n",
    "pareto_counts_df['proportion_pareto'] = pareto_counts_df['n_pareto_solutions'] / pareto_counts_df['total_solutions']\n",
    "\n",
    "\n",
    "# Calculate the average number of Pareto solutions per method\n",
    "avg_pareto_solutions = pareto_counts_df.groupby('method_name')['n_pareto_solutions'].mean().reset_index()\n",
    "\n",
    "# Calculate the average total number of solutions per method\n",
    "avg_total_solutions = pareto_counts_df.groupby('method_name')['total_solutions'].mean().reset_index()\n",
    "\n",
    "# Merge the averages into a single DataFrame\n",
    "avg_counts_df = pd.merge(avg_pareto_solutions, avg_total_solutions, on='method_name')\n",
    "\n",
    "# Sort methods by average total solutions for better visualization\n",
    "avg_counts_df = avg_counts_df.sort_values('total_solutions')\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=False)\n",
    "\n",
    "# Plot average number of Pareto solutions\n",
    "sns.barplot(\n",
    "    data=avg_counts_df,\n",
    "    x='method_name',\n",
    "    y='n_pareto_solutions',\n",
    "    ax=axes[0],\n",
    "    palette='Blues_d'\n",
    ")\n",
    "axes[0].set_title('Average Number of Pareto Solutions per Method')\n",
    "axes[0].set_xlabel('Method Name')\n",
    "axes[0].set_ylabel('Average Number of Pareto Solutions')\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Plot average total number of solutions\n",
    "sns.barplot(\n",
    "    data=avg_counts_df,\n",
    "    x='method_name',\n",
    "    y='total_solutions',\n",
    "    ax=axes[1],\n",
    "    palette='Greens_d'\n",
    ")\n",
    "axes[1].set_title('Average Total Number of Solutions per Method')\n",
    "axes[1].set_xlabel('Method Name')\n",
    "axes[1].set_ylabel('Average Total Number of Solutions')\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF with the best solution per task_id, fold, seed and method\n",
    "print(\"Picking best solutions...\")\n",
    "idx = df.groupby([\"method_name\", \"task_id\", \"fold\", \"seed\"])[\"roc_auc_val\"].idxmax()\n",
    "\n",
    "# Use these indices to get the rows with the maximum 'roc_auc_val' for each group\n",
    "best_val_scores = df.loc[idx]\n",
    "\n",
    "print(\"Averaging over folds...\")\n",
    "avg_over_folds = (\n",
    "    best_val_scores.groupby([\"task_id\", \"method_name\", \"seed\"])\n",
    "    .agg(\n",
    "        {\n",
    "            # \"normalized_memory\": \"mean\",\n",
    "            # \"normalized_diskspace\": \"mean\",\n",
    "            \"roc_auc_val\": \"mean\",\n",
    "            \"roc_auc_test\": \"mean\",\n",
    "            \"inference_time\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "print(\"Averaging over seeds...\")\n",
    "avg_over_seeds = (\n",
    "    avg_over_folds.groupby([\"task_id\", \"method_name\"])\n",
    "    .agg(\n",
    "        {\n",
    "            # \"normalized_memory\": \"mean\",\n",
    "            # \"normalized_diskspace\": \"mean\",\n",
    "            \"roc_auc_val\": \"mean\",\n",
    "            \"roc_auc_test\": \"mean\",\n",
    "            \"inference_time\": \"mean\"\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Plot boxplot for inference time and performance\n",
    "print(f\"Shape after averaging: {avg_over_seeds.shape}\")\n",
    "\n",
    "# Rank data within each task based on 'roc_auc_test' and add as a new column\n",
    "avg_over_seeds[\"rank\"] = avg_over_seeds.groupby(\"task_id\")[\"roc_auc_test\"].rank(\n",
    "    \"dense\", ascending=False\n",
    ")\n",
    "boxplot(avg_over_seeds, \"rank\", flip_y_axis=True)\n",
    "avg_over_seeds[\"negated_roc_auc_test\"] = 1 - avg_over_seeds[\"roc_auc_test\"]\n",
    "pivot_ranks = avg_over_seeds.pivot(\n",
    "    index=\"task_id\", columns=\"method_name\", values=\"negated_roc_auc_test\"\n",
    ")\n",
    "cd_evaluation(\n",
    "    pivot_ranks,\n",
    "    maximize_metric=False,\n",
    "    plt_title=\"Rankings Critical Difference Plot\",\n",
    "    filename=\"../plots/CDPRankings.pdf\",\n",
    ")\n",
    "cd_evaluation(\n",
    "    pivot_ranks,\n",
    "    maximize_metric=False,\n",
    "    plt_title=\"Rankings Critical Difference Plot\",\n",
    "    filename=\"../plots/CDPRankings.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(\n",
    "    df,\n",
    "    \"inference_time\",\n",
    "    log_x_scale=False,\n",
    "    orient=\"h\",\n",
    "    rotation_x_ticks=0,\n",
    "    outliers=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher variance for Multi-GES due to varying time weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latex Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latex_table(df, repo, filename=\"table.tex\", max_char=15):\n",
    "    methods = df[\"method_name\"].unique()\n",
    "    task_ids = df[\"task_id\"].unique()\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{longtable}{l\" + \"c\" * len(methods) + \"}\\n\")\n",
    "        f.write(\n",
    "            \"\\\\caption{Test ROC AUC - Binary: The mean and standard deviation of the test score over all folds for each method. The best methods per dataset are shown in bold. All methods close to the best method are considered best (using NumPys default \\\\texttt{isclose} function).}\\n\"\n",
    "        )\n",
    "        f.write(\"\\\\label{tab:results} \\\\\\\\ \\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\"Dataset & \" + \" & \".join(map(str, methods)) + \" \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\"\\\\endfirsthead\\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\"Dataset & \" + \" & \".join(map(str, methods)) + \" \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\"\\\\endhead\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\n",
    "            \"\\\\multicolumn{\"\n",
    "            + str(len(methods) + 1)\n",
    "            + \"}{r}{Continued on next page} \\\\\\\\\\n\"\n",
    "        )\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\"\\\\endfoot\\n\")\n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\endlastfoot\\n\")\n",
    "\n",
    "        for task_id in task_ids:\n",
    "            dataset_name = repo.tid_to_dataset(\n",
    "                task_id\n",
    "            )  # Convert task_id to dataset name\n",
    "            truncated_name = (\n",
    "                (dataset_name[:max_char] + \"...\")\n",
    "                if len(dataset_name) > max_char\n",
    "                else dataset_name\n",
    "            )\n",
    "            escaped_name = truncated_name.replace(\"_\", \"\\\\_\")  # Escape underscores\n",
    "            line = [str(escaped_name)]  # Ensure the first item is a string\n",
    "            method_scores = []\n",
    "\n",
    "            for method in methods:\n",
    "                method_data = df[\n",
    "                    (df[\"task_id\"] == task_id) & (df[\"method_name\"] == method)\n",
    "                ]\n",
    "                if not method_data.empty:\n",
    "                    mean_score = method_data[\"roc_auc_test\"].mean()\n",
    "                    std_dev = method_data[\"roc_auc_test\"].std()\n",
    "                    score_str = f\"{mean_score:.4f}($\\\\pm${std_dev:.4f})\"\n",
    "                    method_scores.append((mean_score, score_str))\n",
    "                else:\n",
    "                    method_scores.append((None, \"-\"))\n",
    "\n",
    "            # Determine the best score\n",
    "            best_score = max(\n",
    "                score[0] for score in method_scores if score[0] is not None\n",
    "            )\n",
    "\n",
    "            for mean_score, score_str in method_scores:\n",
    "                if mean_score is not None and np.isclose(mean_score, best_score):\n",
    "                    line.append(f\"\\\\textbf{{{score_str}}}\")\n",
    "                else:\n",
    "                    line.append(score_str)\n",
    "\n",
    "            f.write(\" & \".join(line) + \" \\\\\\\\\\n\")\n",
    "\n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\end{longtable}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../tables\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "repo = load_repository(\"D244_F3_C1530_100\", cache=True)\n",
    "create_latex_table(df, repo, filename=\"../tables/table.tex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
