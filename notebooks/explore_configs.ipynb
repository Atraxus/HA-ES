{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Configs in TabRepo\n",
    "\n",
    "This notebook is an explorative analysis of the Configs used in TabRepo. The goal is to find parameters that are good at simulating, emulating or approximating the following hardware-aware metrics:\n",
    "\n",
    "- FLOPs (~ Inference Time)\n",
    "- Latency (~ Inference Time)\n",
    "- Energy Consumption\n",
    "- Memory Footprint\n",
    "- Area\n",
    "\n",
    "\n",
    "## Additional Notes to each Metric\n",
    "\n",
    "### FLOPs\n",
    "--\n",
    "\n",
    "### Latency\n",
    "This is already mostly covered by TabRepos measured inference times. \n",
    "This could be improved upon by comparing this to actual inference times on CPU vs GPU. \n",
    "I would need different hardware to create meaningful data. \n",
    "\n",
    "> Is there research I could use to find good estimates? \n",
    ">\n",
    "> What hardware was used for TabRepo?\n",
    "\n",
    "Further, there are apparently simulators or hardware emulators, which I could use to measure inference times on specific hardware. \n",
    "I do not know at the moment how good the created data would be.\n",
    "\n",
    "### Energy Consumption\n",
    "One option here is to estimate the energy consumption of a model based on existing profiling tools. \n",
    "I am not sure how this would work and what tools there are, so this would require a lot of research.\n",
    "\n",
    "If I have enough data for a couple of representative models, I could create an Energy Model through regression. \n",
    "Through this model I could estimate the other models' energy consumption. \n",
    "I could use the inference and training times from TabRepo as input for the model.\n",
    "\n",
    "### Memory Footprint\n",
    "--\n",
    "\n",
    "### Area\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON configuration files\n",
    "config_path = \"../extern/tabrepo/data/configs/\"\n",
    "files = {\n",
    "    \"xt\": \"configs_xt.json\",\n",
    "    \"xgboost\": \"configs_xgboost.json\",\n",
    "    \"tabpfn\": \"configs_tabpfn.json\",\n",
    "    \"rf\": \"configs_rf.json\",\n",
    "    \"nn_torch\": \"configs_nn_torch.json\",\n",
    "    \"lr\": \"configs_lr.json\",\n",
    "    \"lightgbm\": \"configs_lightgbm.json\",\n",
    "    \"knn\": \"configs_knn.json\",\n",
    "    \"ftt\": \"configs_ftt.json\",\n",
    "    \"fastai\": \"configs_fastai.json\",\n",
    "    \"catboost\": \"configs_catboost.json\"\n",
    "}\n",
    "\n",
    "# Function to load JSON data\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Load all data\n",
    "data =  {name_id: load_json(config_path + filename) for name_id, filename in files.items()}\n",
    "\n",
    "# Function to convert JSON data to DataFrame\n",
    "def json_to_df(data, model_name):\n",
    "    records = []\n",
    "    for config_name, config_data in data.items():\n",
    "        record = {\"model\": model_name, \"config\": config_name}\n",
    "        record.update(config_data[\"hyperparameters\"])\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Convert all JSON data to DataFrames\n",
    "dfs = [json_to_df(config_data, model_name) for model_name, config_data in data.items()]\n",
    "for i, df in enumerate(dfs):\n",
    "    print(f\"Shape of ({i}): {df.shape}\")\n",
    "    print(f\"\\tColumns: {list(df.columns)}\")\n",
    "    # Calculate total entries\n",
    "    total_entries = df.shape[0] * df.shape[1]\n",
    "\n",
    "    # Count non-NaN values\n",
    "    non_nan_count = df.count().sum()\n",
    "\n",
    "    # Count NaN values\n",
    "    nan_count = df.isna().sum().sum()\n",
    "\n",
    "    # Calculate sparsity metrics\n",
    "    percent_non_nan = (non_nan_count / total_entries) * 100\n",
    "    percent_nan = (nan_count / total_entries) * 100\n",
    "\n",
    "    print(f'\\tTotal entries: {total_entries}')\n",
    "    print(f'\\tNon-NaN entries: {non_nan_count} ({percent_non_nan:.2f}%)')\n",
    "    print(f'\\tNaN entries: {nan_count} ({percent_nan:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for NaNs in the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_nans(df: pd.DataFrame):\n",
    "    for column in df.columns:\n",
    "        if column != 'config':  # Exclude the 'model' column from NaN check\n",
    "            # Find rows where the current column is NaN\n",
    "            nan_rows = df[column].isna()\n",
    "            \n",
    "            # Print the column name, row index, and 'model' field for each NaN\n",
    "            for index, is_nan in nan_rows.items():\n",
    "                if is_nan:\n",
    "                    print(f'Column: {column}, Row Index: {index}, Config: {df.loc[index, \"config\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(dfs):\n",
    "    print(f\"Finding NaNs for ({i})\")\n",
    "    find_nans(df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabrepo import load_repository\n",
    "repo = load_repository(\"D244_F3_C1530_100\", cache=True)\n",
    "metrics = repo.metrics(datasets=repo.datasets(), configs=repo.configs())\n",
    "metrics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Australian\"\n",
    "model = \"XGBoost_r97_BAG_L1\"\n",
    "fold = 0\n",
    "metrics.loc[(dataset, fold, model)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN values usually appear in the first few rows of each dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instancing Models from Configs\n",
    "\n",
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tracemalloc\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "model_dir = \"../models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Generate random data\n",
    "num_samples = 1000\n",
    "num_features = 10\n",
    "X_random = np.random.random((num_samples, num_features))\n",
    "y_random = np.random.randint(2, size=num_samples)\n",
    "\n",
    "train_data = pd.DataFrame(X_random, columns=[f'feature_{i}' for i in range(num_features)])\n",
    "train_data['target'] = y_random\n",
    "\n",
    "# Initialize tracemalloc to measure memory usage\n",
    "tracemalloc.start()\n",
    "\n",
    "# Initialize and train the CatBoost model with minimal output\n",
    "model = CatBoostClassifier(\n",
    "    depth=8,\n",
    "    grow_policy='Depthwise',\n",
    "    l2_leaf_reg=3.860757465489678,\n",
    "    learning_rate=0.030421683021409185,\n",
    "    max_ctr_complexity=4,\n",
    "    one_hot_max_size=10,\n",
    "    verbose=0  # Reduce output\n",
    ")\n",
    "\n",
    "model.fit(train_data.drop(columns='target'), train_data['target'])\n",
    "\n",
    "# Measure memory usage after training\n",
    "snapshot = tracemalloc.take_snapshot()\n",
    "top_stats = snapshot.statistics('lineno')\n",
    "memory_usage = sum(stat.size for stat in top_stats)\n",
    "\n",
    "# Save the model to the specified directory\n",
    "model_path = os.path.join(model_dir, 'random_model.cbm')\n",
    "model.save_model(model_path)\n",
    "\n",
    "# Measure the disk usage of the saved model\n",
    "disk_usage = os.path.getsize(model_path)\n",
    "\n",
    "# Output the memory and disk usage\n",
    "print(f\"Memory used by the model: {memory_usage} bytes\")\n",
    "print(f\"Disk space used by the model: {disk_usage} bytes\")\n",
    "print(f\"Model saved at: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2\n",
    "\n",
    "This has been moved to a containerized version for reduced variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tracemalloc\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load the configurations from the JSON files\n",
    "config_path = \"../extern/tabrepo/data/configs/\"\n",
    "files = {\n",
    "    \"xt\": \"configs_xt.json\",\n",
    "    \"xgboost\": \"configs_xgboost.json\",\n",
    "    \"tabpfn\": \"configs_tabpfn.json\",\n",
    "    \"rf\": \"configs_rf.json\",\n",
    "    \"nn_torch\": \"configs_nn_torch.json\",\n",
    "    \"lr\": \"configs_lr.json\",\n",
    "    \"lightgbm\": \"configs_lightgbm.json\",\n",
    "    \"knn\": \"configs_knn.json\",\n",
    "    \"ftt\": \"configs_ftt.json\",\n",
    "    \"fastai\": \"configs_fastai.json\",\n",
    "    \"catboost\": \"configs_catboost.json\"\n",
    "}\n",
    "\n",
    "# Combine configurations into a single dictionary\n",
    "raw_hyperparameters = {}\n",
    "for model, filename in files.items():\n",
    "    with open(os.path.join(config_path, filename), 'r') as file:\n",
    "        raw_hyperparameters.update(json.load(file))\n",
    "\n",
    "# Adjust the configurations using the provided adjustment code logic\n",
    "adjusted_hyperparameters = {}\n",
    "configs_hps = raw_hyperparameters.copy()  # Assuming repo._zeroshot_context.configs_hyperparameters equivalent\n",
    "portfolio_configs = list(raw_hyperparameters.keys())  # Assuming portfolio_configs is a list of all config names\n",
    "\n",
    "for _config_prio, config in enumerate(portfolio_configs):\n",
    "    tabrepo_config_name = config.replace(\"_BAG_L1\", \"\")\n",
    "    new_config = configs_hps[tabrepo_config_name].copy()\n",
    "    model_type = new_config.pop(\"model_type\")\n",
    "    new_config = new_config[\"hyperparameters\"]\n",
    "\n",
    "    if model_type not in adjusted_hyperparameters:\n",
    "        adjusted_hyperparameters[model_type] = []\n",
    "    new_config[\"ag_args\"] = new_config.get(\"ag_args\", {})\n",
    "    new_config[\"ag_args\"][\"priority\"] = 0 - _config_prio\n",
    "    adjusted_hyperparameters[model_type].append(new_config)\n",
    "\n",
    "# Create dummy data\n",
    "num_samples = 100\n",
    "num_features = 10\n",
    "X_dummy = pd.DataFrame(np.random.random((num_samples, num_features)), columns=[f'feature_{i}' for i in range(num_features)])\n",
    "X_dummy['target'] = np.random.randint(2, size=num_samples)\n",
    "\n",
    "# Prepare a list to collect results\n",
    "results = []\n",
    "\n",
    "# Measure memory and disk usage for each model type\n",
    "for model_name, configs in tqdm(adjusted_hyperparameters.items(), desc=\"Model Types\", leave=False):\n",
    "    for i, config in enumerate(tqdm(configs, desc=f\"{model_name} Configurations\", leave=False)):\n",
    "        try:\n",
    "            tracemalloc.start()\n",
    "            \n",
    "            # Initialize and train the model on dummy data\n",
    "            predictor = TabularPredictor(label='target', problem_type='binary', verbosity=0)\n",
    "            predictor.fit(\n",
    "                train_data=X_dummy,\n",
    "                hyperparameters={f\"{model_name}\": config},\n",
    "                time_limit=60,  # short time limit for fitting on dummy data\n",
    "                verbosity=0,\n",
    "            )\n",
    "            \n",
    "            # Measure memory usage\n",
    "            snapshot = tracemalloc.take_snapshot()\n",
    "            top_stats = snapshot.statistics('lineno')\n",
    "            memory_usage = sum(stat.size for stat in top_stats)\n",
    "\n",
    "            # Save the model\n",
    "            predictor.save()\n",
    "            \n",
    "            # Measure disk usage\n",
    "            predictor_file = os.path.join(predictor.path, \"predictor.pkl\")\n",
    "            learner_file = os.path.join(predictor.path, \"learner.pkl\")\n",
    "            model_files = []\n",
    "            for root, dirs, files in os.walk(predictor.path):\n",
    "                for file in files:\n",
    "                    if file == \"model.pkl\":\n",
    "                        model_files.append(os.path.join(root, file))\n",
    "                        \n",
    "            predictor_size = os.path.getsize(predictor_file)\n",
    "            learner_size = os.path.getsize(learner_file)\n",
    "            models_size = sum(os.path.getsize(f) for f in model_files)\n",
    "            total_deployed_size = predictor_size + learner_size + models_size\n",
    "\n",
    "            # Append the results to the list\n",
    "            results.append({\n",
    "                \"Model\": f\"{model_name}_{i}\",\n",
    "                \"Memory used (bytes)\": memory_usage,\n",
    "                \"Predictor size (bytes)\": predictor_size,\n",
    "                \"Learner size (bytes)\": learner_size,\n",
    "                \"Models size (bytes)\": models_size,\n",
    "                \"Total deployment size (bytes)\": total_deployed_size\n",
    "            })\n",
    "            \n",
    "            tracemalloc.stop()\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"Error with model {model_name}_{i}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"General error with model {model_name}_{i}: {e}\")\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_results.to_csv('model_memory_and_disk_usage.csv', index=False)\n",
    "\n",
    "# Output the first few rows for debugging purposes\n",
    "print(df_results.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cPickle\n",
    "\n",
    "Inspecting the pkl files Autogluon creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = './AutogluonModels/ag-20240812_092137/models/ExtraTrees_c1/model.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "print(data)\n",
    "dir(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HA-ES",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
