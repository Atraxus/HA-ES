{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HA-ES Plotting\n",
    "\n",
    "- udpated version of plotting script used for paper \"...\"\n",
    "\n",
    "## General\n",
    "\n",
    "- imports\n",
    "- defintions\n",
    "- loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygmo as pg\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "from autorank._util import get_sorted_rank_groups\n",
    "from autorank import autorank, plot_stats\n",
    "from tabrepo import load_repository\n",
    "\n",
    "method_id_name_dict = {\n",
    "    \"GES\": \"GES*\",\n",
    "    \"SINGLE_BEST\": \"Single-Best\",\n",
    "    \"QO\": \"QO-ES\",\n",
    "    \"QDO\": \"QDO-ES\",\n",
    "    \"ENS_SIZE_QDO\": \"Ensemble Size\",\n",
    "    #\"INFER_TIME_QDO\": \"Inference Time\",\n",
    "    \"INFER_TIME_QDO\": \"HAPEns\",\n",
    "    \"MEMORY_QDO\": \"Memory\",\n",
    "    \"DISK_QDO\": \"Diskspace\",\n",
    "}\n",
    "infer_time_weights = np.linspace(0, 1, num=20)[1:]\n",
    "infer_time_weights = np.round(infer_time_weights, 2)\n",
    "multi_ges_method_ids = [f\"MULTI_GES-{time_weight:.2f}\" for time_weight in infer_time_weights]\n",
    "multi_ges_method_names = [f\"Multi-GES({time_weight:.2f})\" for time_weight in infer_time_weights]\n",
    "for id, name in zip(multi_ges_method_ids, multi_ges_method_names):\n",
    "    method_id_name_dict[id] = name\n",
    "\n",
    "print(\"Loading data. This might take a while...\")\n",
    "df = pd.read_csv(\"../data/full.csv\")\n",
    "\n",
    "# Map method IDs to names\n",
    "method_id_name_dict[\"MULTI_GES-0.79\"] = \"Multi-GES\"\n",
    "if \"method\" in df.columns:\n",
    "    df[\"method_name\"] = df[\"method\"].map(method_id_name_dict)\n",
    "else:\n",
    "    raise ValueError(\"Column 'method' not found in DataFrame\")\n",
    "df = df.dropna(subset=['method_name'])\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df[\"method_name\"].unique())\n",
    "print(df[\"method\"].unique())\n",
    "\n",
    "df[\"models_used_length\"] = df[\"models_used\"].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some initial checks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_task_df = df[df['task'] == df['task'].unique()[0]]\n",
    "print(single_task_df[['normalized_roc_auc_test', 'normalized_time', 'normalized_memory', 'normalized_diskspace']].describe())\n",
    "df.info()\n",
    "\n",
    "print(\"--- Sanity Check: Number of entries per method ---\")\n",
    "entry_counts = df.groupby('method_name').size()\n",
    "print(\"Number of entries (solutions) per method:\")\n",
    "print(entry_counts)\n",
    "print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
    "\n",
    "\n",
    "print(\"--- Sanity Check: Number of unique datasets per method ---\")\n",
    "tasks_per_method = df.groupby('method_name')['task_id'].nunique()\n",
    "print(\"Number of unique tasks evaluated per method:\")\n",
    "print(tasks_per_method)\n",
    "print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
    "\n",
    "\n",
    "print(\"--- Sanity Check: Number of unique seeds per method ---\")\n",
    "seeds_per_method = df.groupby('method_name')['seed'].nunique()\n",
    "print(\"Number of unique seeds evaluated per method:\")\n",
    "print(seeds_per_method)\n",
    "print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
    "\n",
    "\n",
    "print(\"--- Sanity Check: Number of unique folds per method ---\")\n",
    "folds_per_method = df.groupby('method_name')['fold'].nunique()\n",
    "print(\"Number of unique folds evaluated per method:\")\n",
    "print(folds_per_method)\n",
    "print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
    "\n",
    "# --- Verification ---\n",
    "# Check if all methods have the same number of unique tasks, seeds, and folds\n",
    "all_tasks_consistent = tasks_per_method.nunique() == 1\n",
    "all_seeds_consistent = seeds_per_method.nunique() == 1\n",
    "all_folds_consistent = folds_per_method.nunique() == 1\n",
    "\n",
    "if all_tasks_consistent and all_seeds_consistent and all_folds_consistent:\n",
    "    print(\"Verification PASSED: All methods were run on the same number of tasks, seeds, and folds.\")\n",
    "else:\n",
    "    print(\"Verification FAILED: There is an inconsistency in the number of tasks, seeds, or folds across methods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "### Boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def extract_numeric_part(method_name):\n",
    "    \"\"\"\n",
    "    Extracts the numeric part from a method name string. If no numeric part is found, returns None.\n",
    "    \"\"\"\n",
    "    if isinstance(method_name, str):\n",
    "        match = re.search(r\"\\((\\d*\\.?\\d+)\\)\", method_name)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "    return None\n",
    "\n",
    "def boxplot(\n",
    "    df: pd.DataFrame,\n",
    "    y_str: str,\n",
    "    log_y_scale: bool = False,\n",
    "    log_x_scale: bool = False,\n",
    "    flip_y_axis: bool = False,\n",
    "    orient: str = \"v\",\n",
    "    rotation_x_ticks: int = 45,\n",
    "    outliers=False,\n",
    "    sort_by_median: bool = True,  # <-- added flag\n",
    "):\n",
    "    if y_str not in df.columns:\n",
    "        raise ValueError(f\"Column '{y_str}' not found in DataFrame\")\n",
    "    \n",
    "    df[\"method_name\"] = df[\"method_name\"].astype(str)\n",
    "\n",
    "    if sort_by_median:\n",
    "        # Sort by median of the target column\n",
    "        medians = (\n",
    "            df.groupby(\"method_name\")[y_str]\n",
    "            .median()\n",
    "            .sort_values(ascending=False)  # largest median first\n",
    "        )\n",
    "        df[\"method_name\"] = pd.Categorical(df[\"method_name\"], categories=medians.index, ordered=True)\n",
    "        df = df.sort_values(\"method_name\")\n",
    "    else:\n",
    "        # Alphabetic + numeric sorting as fallback\n",
    "        df[\"alphabetic\"] = df[\"method_name\"].apply(lambda x: re.split(r\"\\(\\d*\\.?\\d+\\)\", x)[0])\n",
    "        df[\"numeric\"] = df[\"method_name\"].apply(extract_numeric_part)\n",
    "        df = df.sort_values(by=[\"alphabetic\", \"numeric\"], ascending=[True, True])\n",
    "        df = df.drop(columns=[\"alphabetic\", \"numeric\"])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if orient == \"v\":\n",
    "        sns.boxplot(\n",
    "            data=df,\n",
    "            x=\"method_name\",\n",
    "            y=y_str,\n",
    "            hue=\"method_name\",\n",
    "            palette=\"pastel\",\n",
    "            linewidth=2,\n",
    "            orient=orient,\n",
    "            legend=False,\n",
    "            showfliers=outliers,\n",
    "        )\n",
    "    elif orient == \"h\":\n",
    "        sns.boxplot(\n",
    "            data=df,\n",
    "            x=y_str,\n",
    "            y=\"method_name\",\n",
    "            hue=\"method_name\",\n",
    "            palette=\"pastel\",\n",
    "            linewidth=2,\n",
    "            orient=orient,\n",
    "            legend=False,\n",
    "            showfliers=outliers,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Orient '{orient}' not supported\")\n",
    "\n",
    "    if orient == \"v\":\n",
    "        plt.ylabel(y_str)\n",
    "        plt.xlabel(\"Ensemble Method\")\n",
    "    else:\n",
    "        plt.xlabel(y_str)\n",
    "        plt.ylabel(\"Ensemble Method\")\n",
    "\n",
    "    if log_y_scale:\n",
    "        plt.yscale(\"log\")\n",
    "    if log_x_scale:\n",
    "        plt.xscale(\"log\")\n",
    "    if flip_y_axis:\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.xticks(rotation=rotation_x_ticks)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    directory = \"../plots\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    plt.savefig(f\"{directory}/boxplot_{y_str}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(f\"{directory}/boxplot_{y_str}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def interactive_boxplot(df, column_options):\n",
    "    y_str_widget = widgets.Dropdown(\n",
    "        options=column_options,\n",
    "        description=\"Y-axis column:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    log_y_scale_widget = widgets.Checkbox(value=False, description=\"Log-scale Y-axis\")\n",
    "    log_x_scale_widget = widgets.Checkbox(value=False, description=\"Log-scale X-axis\")\n",
    "    flip_y_axis_widget = widgets.Checkbox(value=False, description=\"Flip Y-axis\")\n",
    "    orient_widget = widgets.RadioButtons(\n",
    "        options=[\"v\", \"h\"],\n",
    "        value=\"h\",\n",
    "        description=\"Orientation:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "    rotation_x_ticks_widget = widgets.IntSlider(\n",
    "        value=0, min=0, max=90, step=5, description=\"Rotation X-ticks\"\n",
    "    )\n",
    "\n",
    "    ui = widgets.VBox(\n",
    "        [\n",
    "            y_str_widget,\n",
    "            log_y_scale_widget,\n",
    "            log_x_scale_widget,\n",
    "            flip_y_axis_widget,\n",
    "            orient_widget,\n",
    "            rotation_x_ticks_widget,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    out = widgets.interactive_output(\n",
    "        boxplot,\n",
    "        {\n",
    "            \"df\": widgets.fixed(df),\n",
    "            \"y_str\": y_str_widget,\n",
    "            \"log_y_scale\": log_y_scale_widget,\n",
    "            \"log_x_scale\": log_x_scale_widget,\n",
    "            \"flip_y_axis\": flip_y_axis_widget,\n",
    "            \"orient\": orient_widget,\n",
    "            \"rotation_x_ticks\": rotation_x_ticks_widget,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical Difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd_evaluation(\n",
    "    scores,\n",
    "    maximize_metric=True,\n",
    "    plt_title=\"Critical Difference Plot\",\n",
    "    filename=\"CriticalDifferencePlot.png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    scores: DataFrame with method names as columns and tasks as rows, each cell contains a score value.\n",
    "    maximize_metric: Boolean, True if higher values are better.\n",
    "    output_path: Where to save the plot, if None, plot will not be saved.\n",
    "    plt_title: Title of the plot.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    rank_data = -scores if maximize_metric else scores\n",
    "\n",
    "    # Run autorank\n",
    "    result = autorank(rank_data, alpha=0.05, verbose=False, order=\"ascending\")\n",
    "\n",
    "    # Plot with updated font size\n",
    "    plt.close(\"all\")\n",
    "    width = 6\n",
    "    fig, ax = plt.subplots(figsize=(12, width))\n",
    "    plt.rcParams.update({\"font.size\": 20})\n",
    "\n",
    "    plot_stats(result, ax=ax)\n",
    "    ax.tick_params(axis=\"both\", labelsize=20)  # Set font size for axis ticks\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    ax.set_xticklabels(labels, fontsize=20)  # Adjust fontsize as needed\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(filename, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_options = [\"models_used_length\", \"normalized_roc_auc_test\", \"normalized_roc_auc_val\", \"inference_time\", \"memory\", \"diskspace\", \"normalized_memory\"]\n",
    "\n",
    "#filtered_df = df[~df['method_name'].isin(['Multi-GES(0.43)', 'Multi-GES(0.50)'])].copy()\n",
    "interactive_boxplot(df, column_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mticker\n",
    "# Group the DataFrame by 'method_name' and compute total and unique counts\n",
    "result_df = df.groupby('method_name').agg(\n",
    "    total_solutions=('models_used', 'count'),\n",
    "    unique_solutions=('models_used', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate the percentage of unique solutions\n",
    "result_df['percentage_unique_solutions'] = (\n",
    "    result_df['unique_solutions'] / result_df['total_solutions'] * 100\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by the percentage of unique solutions for clearer visualization\n",
    "result_df = result_df.sort_values('percentage_unique_solutions', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "color_total = 'steelblue'\n",
    "color_unique = 'indianred'\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.bar(result_df['method_name'], result_df['total_solutions'], color=color_total, label='Total Solutions')\n",
    "\n",
    "# --- Y-axis 1: Total Solutions ---\n",
    "ax.set_ylabel(\"Total Solutions\", color='black', fontsize=20, labelpad=15)\n",
    "ax.tick_params(axis='y', labelcolor='black', pad=5, labelsize=18)\n",
    "ax.grid(axis='y', linestyle='--', color=color_total, alpha=0.4)\n",
    "ax.yaxis.set_major_locator(mticker.MaxNLocator(nbins=6, integer=True))\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(result_df['method_name'], result_df['percentage_unique_solutions'], color=color_unique, marker='o', linestyle='-', linewidth=3, markersize=10, label='Percentage Unique Solutions')\n",
    "\n",
    "# --- Y-axis 2: Percentage Unique ---\n",
    "ax2.set_ylabel('Percentage of Unique Solutions (%)', color='black', fontsize=20, labelpad=15)\n",
    "ax2.tick_params(axis='y', labelcolor='black', pad=5, labelsize=18)\n",
    "ax2.grid(False)\n",
    "ax2.yaxis.set_major_locator(mticker.MaxNLocator(nbins=6))\n",
    "ax2.yaxis.set_major_formatter(mticker.PercentFormatter())\n",
    "\n",
    "# --- Common Settings ---\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=18)\n",
    "\n",
    "# --- Legend ---\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(\n",
    "    lines + lines2,\n",
    "    labels + labels2,\n",
    "    loc='upper right',\n",
    "    fontsize=16,\n",
    "    frameon=True,\n",
    "    facecolor='white',\n",
    "    edgecolor='grey'\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../plots/total-vs-unique-ensembles.png\", dpi=300)\n",
    "plt.savefig(\"../plots/total-vs-unique-ensembles.pdf\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto Front Eval\n",
    "\n",
    "### True Pareto Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParetoFront(df: pd.DataFrame, objectives: list[str], return_mask: bool = False):\n",
    "    \"\"\"\n",
    "    Finds the Pareto-efficient points from a DataFrame.\n",
    "    \n",
    "    This function assumes ALL objectives are to be MINIMIZED.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the points.\n",
    "        objectives (List[str]): List of column names for the objectives.\n",
    "        return_mask (bool): If True, returns a boolean mask. If False, returns the filtered DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or np.ndarray: The Pareto-efficient points or a boolean mask.\n",
    "    \"\"\"\n",
    "    costs = df[objectives].values\n",
    "    is_efficient = np.ones(costs.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(costs):\n",
    "        if is_efficient[i]:\n",
    "            # Find all points not dominated by c\n",
    "            is_efficient[is_efficient] = np.any(costs[is_efficient] < c, axis=1)\n",
    "            # And keep c itself\n",
    "            is_efficient[i] = True\n",
    "    \n",
    "    return is_efficient if return_mask else df[is_efficient]\n",
    "\n",
    "    \n",
    "def calculate_pareto_fronts(df, objectives):\n",
    "    \"\"\"\n",
    "    Calculate the Pareto front points for all solutions in the DataFrame per task_id, seed, and fold.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - objectives: list of two objective column names.\n",
    "\n",
    "    Returns:\n",
    "    - A nested dictionary with structure {task_id: {seed: {fold: pareto_front_df}}}\n",
    "    \"\"\"\n",
    "    if len(objectives) != 2:\n",
    "        raise ValueError(\"Exactly two objectives must be provided.\")\n",
    "\n",
    "    # Initialize a dictionary to store the Pareto fronts\n",
    "    pareto_fronts = {}\n",
    "\n",
    "    # Iterate over unique task_ids\n",
    "    for task_id in df[\"task_id\"].unique():\n",
    "        pareto_fronts[task_id] = {}\n",
    "\n",
    "        # Iterate over unique seeds\n",
    "        for seed in df[\"seed\"].unique():\n",
    "            pareto_fronts[task_id][seed] = {}\n",
    "\n",
    "            # Iterate over unique folds\n",
    "            for fold in df[\"fold\"].unique():\n",
    "                # Filter the DataFrame for the current task_id, seed, and fold\n",
    "                df_fold = df[\n",
    "                    (df[\"task_id\"] == task_id)\n",
    "                    & (df[\"seed\"] == seed)\n",
    "                    & (df[\"fold\"] == fold)\n",
    "                ]\n",
    "\n",
    "                if df_fold.empty:\n",
    "                    continue  # Skip if no data for this combination\n",
    "\n",
    "                # Compute Pareto front for df_fold\n",
    "                is_efficient = getParetoFront(df_fold, objectives, return_mask=True)\n",
    "                pareto_front_df = df_fold[is_efficient]\n",
    "\n",
    "                # Store the Pareto front DataFrame\n",
    "                pareto_fronts[task_id][seed][fold] = pareto_front_df\n",
    "\n",
    "    return pareto_fronts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_data import normalize_data\n",
    "\n",
    "hw_score = 'hw_score'\n",
    "perf_metric = 'normalized_roc_auc_test'\n",
    "hw_metrics = ['normalized_time', 'normalized_memory', 'normalized_diskspace']\n",
    "\n",
    "df['hw_score'] = df[hw_metrics].mean(axis=1)\n",
    "for task in df[\"task\"].unique():\n",
    "    mask = df[\"task\"] == task\n",
    "    if \"hw_score\" in df.columns:\n",
    "        df.loc[mask, \"normalized_hw_score\"] = normalize_data(\n",
    "            df.loc[mask, \"hw_score\"]\n",
    "        )\n",
    "\n",
    "metric_pairs = [(perf_metric, hw_metric) for hw_metric in hw_metrics]\n",
    "metric_pairs.append(('normalized_roc_auc_test', 'hw_score'))\n",
    "metric_pairs.append(('normalized_roc_auc_test', 'normalized_hw_score'))\n",
    "print(metric_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypervolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_hypervolumes(\n",
    "    df: pd.DataFrame,\n",
    "    method_name: str,\n",
    "    perf_metric: str,\n",
    "    hw_metrics: list[str],\n",
    ") -> dict:\n",
    "    print(f\"Calculating for HV for {method_name} and metrics {perf_metric} {hw_metrics}\")\n",
    "    all_metrics = [perf_metric] + hw_metrics\n",
    "    ref_point = [1.01] + [1.01] * len(hw_metrics)\n",
    "\n",
    "    df_method = df[df[\"method_name\"] == method_name]\n",
    "    hypervolumes_per_task = {}\n",
    "\n",
    "    # Iterate over unique task_ids\n",
    "    for task_id in df_method[\"task_id\"].unique():\n",
    "        seed_hypervolumes = []  # Store hypervolumes for each seed\n",
    "\n",
    "        for seed in df_method[\"seed\"].unique():\n",
    "            fold_hypervolumes = []  # Store hypervolumes for each fold\n",
    "\n",
    "            for fold in df_method[\"fold\"].unique():\n",
    "                df_fold = df_method[\n",
    "                    (df_method[\"task_id\"] == task_id)\n",
    "                    & (df_method[\"seed\"] == seed)\n",
    "                    & (df_method[\"fold\"] == fold)\n",
    "                ]\n",
    "\n",
    "                if df_fold.empty:\n",
    "                    continue\n",
    "\n",
    "                points = df_fold[all_metrics].copy()\n",
    "                points_to_maximize = points.values\n",
    "                hv = pg.hypervolume(points_to_maximize)\n",
    "                hypervolume = hv.compute(ref_point)\n",
    "                fold_hypervolumes.append(hypervolume)\n",
    "\n",
    "            # Average hypervolumes across all folds for the current seed\n",
    "            if fold_hypervolumes:\n",
    "                average_fold_hypervolume = np.mean(fold_hypervolumes)\n",
    "                seed_hypervolumes.append(average_fold_hypervolume)\n",
    "\n",
    "        # Average the seed-level averages for the current task\n",
    "        if seed_hypervolumes:\n",
    "            average_seed_hypervolume = np.mean(seed_hypervolumes)\n",
    "            hypervolumes_per_task[task_id] = average_seed_hypervolume\n",
    "\n",
    "    return hypervolumes_per_task\n",
    "\n",
    "\n",
    "def plot_hypervolumes(all_hypervolumes, title: str, directory: str = \"../plots/\"):\n",
    "    # Prepare the data for plotting\n",
    "    methods = list(all_hypervolumes.keys())  # Method names\n",
    "    hv_values = [list(all_hypervolumes[method].values()) for method in methods]\n",
    "    data = []\n",
    "\n",
    "    # Creating a DataFrame suitable for Seaborn\n",
    "    for method_index, values in enumerate(hv_values):\n",
    "        for value in values:\n",
    "            data.append({\"Method\": methods[method_index], \"Hypervolume\": value})\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Calculate medians for each method and sort by median\n",
    "    median_order = df.groupby(\"Method\")[\"Hypervolume\"].median().sort_values(ascending=False).index\n",
    "\n",
    "    # Set the figure size and style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Use seaborn's boxplot to plot the DataFrame, ordered by median values\n",
    "    ax = sns.boxplot(\n",
    "        y=\"Method\", x=\"Hypervolume\", data=df, hue=\"Method\", palette=\"Set2\",\n",
    "        orient=\"h\", order=median_order\n",
    "    )\n",
    "\n",
    "    # Set titles and labels\n",
    "    ax.set_ylabel(\"Method\", fontsize=20)\n",
    "    ax.set_xlabel(\"Hypervolume\", fontsize=20)\n",
    "\n",
    "    # Set font size for ticks\n",
    "    ax.tick_params(axis=\"x\", labelrotation=45, labelsize=16)\n",
    "    ax.tick_params(axis=\"y\", labelsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(directory + title + \".png\", dpi=300)\n",
    "    plt.savefig(directory + title + \".pdf\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for perf_metric, hw_metric in metric_pairs:\n",
    "    methods = df[\"method_name\"].unique()\n",
    "    all_hypervolumes = {}\n",
    "\n",
    "    for method in methods:\n",
    "        all_hypervolumes[method] = calculate_average_hypervolumes(df, method, perf_metric=perf_metric, hw_metrics=[hw_metric])\n",
    "\n",
    "    # You might want to make this plot function also save to a dynamic filename\n",
    "    plot_hypervolumes(all_hypervolumes, f\"bp_hv_{perf_metric}_{hw_metric}\")\n",
    "\n",
    "    # Create dynamic filenames to avoid overwriting results\n",
    "    csv_filename = f\"../data/hypervolumes_{perf_metric}_{hw_metric}.csv\"\n",
    "    plot_filename_pdf = f\"../plots/CDP_HV_{perf_metric}_{hw_metric}.pdf\"\n",
    "    plot_filename_png = f\"../plots/CDP_HV_{perf_metric}_{hw_metric}.png\"\n",
    "\n",
    "    hypervolumes_df = pd.DataFrame(all_hypervolumes)\n",
    "    hypervolumes_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Saved hypervolume data to {csv_filename}\")\n",
    "\n",
    "    data = []\n",
    "    for method, task_ids in all_hypervolumes.items():\n",
    "        for task_id, hypervolume in task_ids.items():\n",
    "            data.append({\"Task\": task_id, \"Method\": method, \"Hypervolume\": hypervolume})\n",
    "\n",
    "    df_hypervolumes = pd.DataFrame(data)\n",
    "    pivot_hypervolumes = df_hypervolumes.pivot(\n",
    "        index=\"Task\", columns=\"Method\", values=\"Hypervolume\"\n",
    "    )\n",
    "\n",
    "    # Run CD evaluation with dynamic filenames\n",
    "    hv_result = cd_evaluation(\n",
    "        pivot_hypervolumes,\n",
    "        maximize_metric=True,\n",
    "        plt_title=f\"Hypervolume CD Plot for {perf_metric} vs {hw_metric}\",\n",
    "        filename=plot_filename_pdf,\n",
    "    )\n",
    "    hv_result = cd_evaluation(\n",
    "        pivot_hypervolumes,\n",
    "        maximize_metric=True,\n",
    "        plt_title=f\"Hypervolume CD Plot for {perf_metric} vs {hw_metric}\",\n",
    "        filename=plot_filename_png,\n",
    "    )\n",
    "\n",
    "print(\"\\n--- Analysis complete for all pairs. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = df[\"method_name\"].unique()\n",
    "all_hypervolumes = {}\n",
    "\n",
    "for method in methods:\n",
    "    all_hypervolumes[method] = calculate_average_hypervolumes(df, method, perf_metric=perf_metric, hw_metrics=hw_metrics)\n",
    "\n",
    "# You might want to make this plot function also save to a dynamic filename\n",
    "plot_hypervolumes(all_hypervolumes, f\"bp_hv_{perf_metric}_{hw_metric}\")\n",
    "\n",
    "# Create dynamic filenames to avoid overwriting results\n",
    "csv_filename = f\"../data/hypervolumes_{perf_metric}_hw_metrics.csv\"\n",
    "plot_filename_pdf = f\"../plots/CDP_HV_{perf_metric}_hw_metrics.pdf\"\n",
    "plot_filename_png = f\"../plots/CDP_HV_{perf_metric}_hw_metrics.png\"\n",
    "\n",
    "hypervolumes_df = pd.DataFrame(all_hypervolumes)\n",
    "hypervolumes_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Saved hypervolume data to {csv_filename}\")\n",
    "\n",
    "data = []\n",
    "for method, task_ids in all_hypervolumes.items():\n",
    "    for task_id, hypervolume in task_ids.items():\n",
    "        data.append({\"Task\": task_id, \"Method\": method, \"Hypervolume\": hypervolume})\n",
    "\n",
    "df_hypervolumes = pd.DataFrame(data)\n",
    "pivot_hypervolumes = df_hypervolumes.pivot(\n",
    "    index=\"Task\", columns=\"Method\", values=\"Hypervolume\"\n",
    ")\n",
    "\n",
    "# Run CD evaluation with dynamic filenames\n",
    "hv_result = cd_evaluation(\n",
    "    pivot_hypervolumes,\n",
    "    maximize_metric=True,\n",
    "    plt_title=f\"Hypervolume CD Plot for {perf_metric} vs {hw_metric}\",\n",
    "    filename=plot_filename_pdf,\n",
    ")\n",
    "hv_result = cd_evaluation(\n",
    "    pivot_hypervolumes,\n",
    "    maximize_metric=True,\n",
    "    plt_title=f\"Hypervolume CD Plot for {perf_metric} vs {hw_metric}\",\n",
    "    filename=plot_filename_png,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Generational Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.indicators.igd_plus import IGDPlus\n",
    "\n",
    "def calculate_IGDp_per_method(df, perf_metric: str, hw_metrics: list[str]):\n",
    "    results = []\n",
    "    all_objectives = [perf_metric] + hw_metrics\n",
    "\n",
    "    # Iterate over unique task_ids\n",
    "    for task_id in df[\"task_id\"].unique():\n",
    "        # Iterate over unique seeds\n",
    "        for seed in df[\"seed\"].unique():\n",
    "            # Iterate over unique folds\n",
    "            for fold in df[\"fold\"].unique():\n",
    "                # Filter the DataFrame for the current task_id, seed, and fold\n",
    "                df_fold = df[\n",
    "                    (df[\"task_id\"] == task_id)\n",
    "                    & (df[\"seed\"] == seed)\n",
    "                    & (df[\"fold\"] == fold)\n",
    "                ]\n",
    "\n",
    "                if df_fold.empty:\n",
    "                    continue  # Skip if no data for this combination\n",
    "\n",
    "                # Compute the reference Pareto front (from all methods)\n",
    "                is_efficient = getParetoFront(df_fold, all_objectives, return_mask=True)\n",
    "                reference_pareto_front = df_fold[is_efficient]\n",
    "                pf_points = reference_pareto_front[all_objectives].values\n",
    "\n",
    "                # Iterate over unique method_names\n",
    "                for method_name in df_fold[\"method_name\"].unique():\n",
    "                    df_method = df_fold[df_fold[\"method_name\"] == method_name]\n",
    "\n",
    "                    # Compute Pareto front for df_method\n",
    "                    is_efficient = getParetoFront(df_method, all_objectives, return_mask=True)\n",
    "                    method_pareto_front = df_method[is_efficient]\n",
    "                    method_points = method_pareto_front[all_objectives].values\n",
    "\n",
    "                    if len(method_points) == 0 or len(pf_points) == 0:\n",
    "                        igd_value = np.nan\n",
    "                    else:\n",
    "                        # Calculate IGD\n",
    "                        ind = IGDPlus(pf_points)\n",
    "                        igd_value = ind(method_points)\n",
    "\n",
    "                    # Append results\n",
    "                    results.append({\n",
    "                        \"task_id\": task_id,\n",
    "                        \"seed\": seed,\n",
    "                        \"fold\": fold,\n",
    "                        \"method_name\": method_name,\n",
    "                        \"IGD\": igd_value\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IGD+ per method\n",
    "for perf_metric, hw_metric in metric_pairs:\n",
    "    metric_pair = [perf_metric, hw_metric]\n",
    "    igd_results = calculate_IGDp_per_method(df, perf_metric, [hw_metric])\n",
    "    \n",
    "    # Group by 'method_name' and 'task_id', and aggregate with 'sum'\n",
    "    agg_igd = igd_results.groupby([\"method_name\", \"task_id\"]).agg('sum').reset_index()\n",
    "    igd_pivots = agg_igd.pivot(\n",
    "        index=\"task_id\", columns=\"method_name\", values=\"IGD\"\n",
    "    )\n",
    "\n",
    "    igd_result = cd_evaluation(\n",
    "        igd_pivots,\n",
    "        maximize_metric=False,\n",
    "        plt_title=\"Hypervolume Critical Difference Plot\",\n",
    "        filename=f\"../plots/CDP_IGD_{perf_metric}_{hw_metric}.png\",\n",
    "    )\n",
    "    igd_result = cd_evaluation(\n",
    "        igd_pivots,\n",
    "        maximize_metric=False,\n",
    "        plt_title=\"Hypervolume Critical Difference Plot\",\n",
    "        filename=f\"../plots/CDP_IGD_{perf_metric}_{hw_metric}.pdf\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igd_results = calculate_IGDp_per_method(df, perf_metric, hw_metrics)\n",
    "agg_igd = igd_results.groupby([\"method_name\", \"task_id\"]).agg('sum').reset_index()\n",
    "igd_pivots = agg_igd.pivot(\n",
    "    index=\"task_id\", columns=\"method_name\", values=\"IGD\"\n",
    ")\n",
    "\n",
    "igd_result = cd_evaluation(\n",
    "    igd_pivots,\n",
    "    maximize_metric=False,\n",
    "    plt_title=\"Hypervolume Critical Difference Plot\",\n",
    "    filename=f\"../plots/CDP_IGD_{perf_metric}_hw_metrics.png\",\n",
    ")\n",
    "igd_result = cd_evaluation(\n",
    "    igd_pivots,\n",
    "    maximize_metric=False,\n",
    "    plt_title=\"Hypervolume Critical Difference Plot\",\n",
    "    filename=f\"../plots/CDP_IGD_{perf_metric}_hw_metrics.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "def plot_normalized_improvement(\n",
    "    df_scores: pd.DataFrame,\n",
    "    baseline_method: str,\n",
    "    higher_is_better: bool,\n",
    "    title: str,\n",
    "    directory: str = \"../plots/\",\n",
    "):\n",
    "    # Check if the baseline method exists in the DataFrame\n",
    "    if baseline_method not in df_scores.columns:\n",
    "        print(f\"Warning: Baseline method '{baseline_method}' not found. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    # Extract the baseline scores for comparison\n",
    "    baseline_scores = df_scores[baseline_method]\n",
    "    \n",
    "    # Initialize a new DataFrame to store the improvement values\n",
    "    improvement_df = pd.DataFrame(index=df_scores.index)\n",
    "\n",
    "    # Get a list of methods to compare against the baseline\n",
    "    methods_to_plot = [col for col in df_scores.columns if col != baseline_method]\n",
    "\n",
    "    for method in methods_to_plot:\n",
    "        method_scores = df_scores[method]\n",
    "        \n",
    "        # Calculate the raw improvement based on whether higher or lower scores are better\n",
    "        if higher_is_better:\n",
    "            # Improvement = (Method Score - Baseline Score)\n",
    "            improvement = method_scores - baseline_scores\n",
    "        else:\n",
    "            # Improvement = (Baseline Score - Method Score)\n",
    "            improvement = baseline_scores - method_scores\n",
    "\n",
    "        # Normalize the improvement by the absolute value of the baseline score\n",
    "        # This gives the relative (percentage) improvement.\n",
    "        # A small epsilon (1e-9) is added to avoid division by zero.\n",
    "        normalized_improvement = improvement / (np.abs(baseline_scores) + 1e-9)\n",
    "        improvement_df[method] = normalized_improvement\n",
    "\n",
    "    # Restructure the DataFrame from wide to long format for easy plotting with seaborn\n",
    "    melted_df = improvement_df.melt(var_name=\"Method\", value_name=\"Normalized Improvement\")\n",
    "\n",
    "    # Calculate the median improvement for each method to sort the plot\n",
    "    median_order = melted_df.groupby(\"Method\")[\"Normalized Improvement\"].median().sort_values(ascending=False).index\n",
    "\n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    ax = sns.boxplot(\n",
    "        y=\"Method\",\n",
    "        x=\"Normalized Improvement\",\n",
    "        data=melted_df,\n",
    "        hue=\"Method\",\n",
    "        palette=\"viridis\",\n",
    "        orient=\"h\",\n",
    "        order=median_order,\n",
    "        showfliers=False,  # Outliers can skew the view, so they are hidden for clarity\n",
    "    )\n",
    "\n",
    "    # Add a vertical dashed red line at 0% to represent the baseline performance\n",
    "    ax.axvline(0, color='r', linestyle='--')\n",
    "\n",
    "    # Set plot titles and labels with appropriate font sizes\n",
    "    ax.set_title(title, fontsize=22, pad=20)\n",
    "    ax.set_ylabel(\"Method\", fontsize=18)\n",
    "    ax.set_xlabel(f\"Normalized Improvement Over {baseline_method}\", fontsize=18)\n",
    "    \n",
    "    # Set tick sizes\n",
    "    ax.tick_params(axis=\"x\", labelsize=14)\n",
    "    ax.tick_params(axis=\"y\", labelsize=14)\n",
    "    \n",
    "    # Format the x-axis to show percentages\n",
    "    ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    # Sanitize the title to create a valid filename\n",
    "    filename_title = title.replace(' ', '_').replace('/', '_')\n",
    "    \n",
    "    # Save the plot in both PNG and PDF formats\n",
    "    plt.savefig(f\"{directory}/{filename_title}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(f\"{directory}/{filename_title}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot for Hypervolume ---\n",
    "# Higher is better for Hypervolume, so we set higher_is_better=True\n",
    "plot_normalized_improvement(\n",
    "    df_scores=pivot_hypervolumes,\n",
    "    baseline_method=\"Single-Best\",\n",
    "    higher_is_better=True,\n",
    "    title=\"Normalized Improvement on Hypervolume vs Single-Best\"\n",
    ")\n",
    "\n",
    "# --- Plot for IGD+ ---\n",
    "# Lower is better for IGD+, so we set higher_is_better=False\n",
    "plot_normalized_improvement(\n",
    "    df_scores=igd_pivots,\n",
    "    baseline_method=\"Single-Best\",\n",
    "    higher_is_better=False,\n",
    "    title=\"Normalized Improvement on IGD+ vs Single-Best\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto Front Topography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique method names\n",
    "method_names = df[\"method_name\"].unique()\n",
    "\n",
    "# Calculate Pareto fronts for each method\n",
    "method_pareto_fronts = {}\n",
    "for method in method_names:\n",
    "    df_method = df[df[\"method_name\"] == method]\n",
    "    method_pareto_fronts[method] = calculate_pareto_fronts(df_method, ['normalized_roc_auc_test', 'normalized_time'])\n",
    "\n",
    "# Function to count the number of solutions in Pareto fronts\n",
    "def count_pareto_solutions(pareto_fronts):\n",
    "    counts = {}\n",
    "    for task_id in pareto_fronts:\n",
    "        for seed in pareto_fronts[task_id]:\n",
    "            for fold in pareto_fronts[task_id][seed]:\n",
    "                pareto_front_df = pareto_fronts[task_id][seed][fold]\n",
    "                n_solutions = len(pareto_front_df)\n",
    "                key = (task_id, seed, fold)\n",
    "                counts[key] = n_solutions\n",
    "    return counts\n",
    "\n",
    "# Count solutions in the method-specific Pareto fronts\n",
    "method_pareto_counts = {}\n",
    "for method in method_names:\n",
    "    method_pareto_counts[method] = count_pareto_solutions(method_pareto_fronts[method])\n",
    "\n",
    "# Calculate total number of solutions per method, task_id, seed, fold\n",
    "total_counts = df.groupby(['method_name', 'task_id', 'seed', 'fold']).size().reset_index(name='total_solutions')\n",
    "\n",
    "# Prepare data for plotting counts\n",
    "data_list = []\n",
    "\n",
    "# Add the method-specific Pareto fronts and total counts\n",
    "for method in method_names:\n",
    "    counts = method_pareto_counts[method]\n",
    "    for key, n_solutions in counts.items():\n",
    "        task_id, seed, fold = key\n",
    "        # Get total number of solutions for this method, task_id, seed, fold\n",
    "        total_solutions = total_counts[\n",
    "            (total_counts['method_name'] == method) &\n",
    "            (total_counts['task_id'] == task_id) &\n",
    "            (total_counts['seed'] == seed) &\n",
    "            (total_counts['fold'] == fold)\n",
    "        ]['total_solutions'].values[0]\n",
    "        data_list.append({\n",
    "            'method_name': method,\n",
    "            'task_id': task_id,\n",
    "            'seed': seed,\n",
    "            'fold': fold,\n",
    "            'n_pareto_solutions': n_solutions,\n",
    "            'total_solutions': total_solutions\n",
    "        })\n",
    "\n",
    "pareto_counts_df = pd.DataFrame(data_list)\n",
    "pareto_counts_df['proportion_pareto'] = pareto_counts_df['n_pareto_solutions'] / pareto_counts_df['total_solutions']\n",
    "\n",
    "\n",
    "# Calculate the average number of Pareto solutions per method\n",
    "avg_pareto_solutions = pareto_counts_df.groupby('method_name')['n_pareto_solutions'].mean().reset_index()\n",
    "\n",
    "# Calculate the average total number of solutions per method\n",
    "avg_total_solutions = pareto_counts_df.groupby('method_name')['total_solutions'].mean().reset_index()\n",
    "\n",
    "# Merge the averages into a single DataFrame\n",
    "avg_counts_df = pd.merge(avg_pareto_solutions, avg_total_solutions, on='method_name')\n",
    "\n",
    "# Sort methods by average total solutions for better visualization\n",
    "avg_counts_df = avg_counts_df.sort_values('total_solutions')\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=False)\n",
    "\n",
    "# Plot average number of Pareto solutions\n",
    "sns.barplot(\n",
    "    data=avg_counts_df,\n",
    "    x='method_name',\n",
    "    y='n_pareto_solutions',\n",
    "    ax=axes[0],\n",
    "    palette='Blues_d'\n",
    ")\n",
    "axes[0].set_title(None)\n",
    "axes[0].set_xlabel(None)\n",
    "axes[0].set_ylabel('Average Number of Pareto Solutions', fontsize=14)\n",
    "axes[0].tick_params(axis='x', rotation=90, labelsize=14)\n",
    "axes[0].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "# Plot average total number of solutions\n",
    "sns.barplot(\n",
    "    data=avg_counts_df,\n",
    "    x='method_name',\n",
    "    y='total_solutions',\n",
    "    ax=axes[1],\n",
    "    palette='Greens_d'\n",
    ")\n",
    "axes[1].set_title(None)\n",
    "axes[1].set_xlabel(None)\n",
    "axes[1].set_ylabel('Average Total Number of Solutions', fontsize=14)\n",
    "axes[1].tick_params(axis='x', rotation=90, labelsize=14)\n",
    "axes[1].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../plots/avg_good_pareto_vs_total_ensembles.png\", dpi=300)\n",
    "plt.savefig(\"../plots/avg_good_pareto_vs_total_ensembles.pdf\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of Solutions on the Global Pareto Front\n",
    "\n",
    "The following plot illustrates the effectiveness of each method in generating solutions that are part of the global Pareto front. For each task, the global Pareto front is determined by considering all solutions from all methods. We then calculate the proportion of each method's generated solutions that lie on this global front. A higher proportion indicates that a method is more efficient at finding globally optimal solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "print(\"Preparing data for per-task and per-seed analysis...\")\n",
    "df_main = df.copy()\n",
    "task_column_name = 'task'\n",
    "\n",
    "cost_metrics = ['normalized_time', 'normalized_memory', 'normalized_diskspace']\n",
    "unique_tasks = df_main[task_column_name].unique()\n",
    "unique_seeds = df_main['seed'].unique()\n",
    "all_proportions = []\n",
    "\n",
    "# Loop over each task\n",
    "for task in unique_tasks:\n",
    "    # --- CHANGE: Added inner loop to iterate over each seed ---\n",
    "    for seed in unique_seeds:\n",
    "        # Filter for the specific task and seed combination\n",
    "        df_task_seed = df_main[(df_main[task_column_name] == task) & (df_main['seed'] == seed)].copy()\n",
    "\n",
    "        # If there's no data for this combination, skip to the next\n",
    "        if df_task_seed.empty:\n",
    "            continue\n",
    "\n",
    "        # Get total solutions for this specific task-seed combo\n",
    "        total_solutions_task_seed = df_task_seed.groupby('method_name', observed=True).size()\n",
    "\n",
    "        for metric in cost_metrics:\n",
    "            objectives = ['normalized_roc_auc_test', metric]\n",
    "            pf_col_name = f'is_on_pf_{metric}'\n",
    "            df_task_seed[pf_col_name] = False\n",
    "\n",
    "            # Calculate Pareto front for each fold within this task and seed\n",
    "            global_pareto_indices = []\n",
    "            # --- CHANGE: Grouping is now only by 'fold' ---\n",
    "            for name, group in df_task_seed.groupby(['fold']):\n",
    "                if not group.empty:\n",
    "                    is_efficient_mask = getParetoFront(group, objectives, return_mask=True)\n",
    "                    global_pareto_indices.extend(group[is_efficient_mask].index)\n",
    "\n",
    "            # Ensure we only use unique indices\n",
    "            df_task_seed.loc[list(set(global_pareto_indices)), pf_col_name] = True\n",
    "\n",
    "            # Calculate proportions for each method within this task-seed combo\n",
    "            solutions_on_pf = df_task_seed.groupby('method_name', observed=True)[pf_col_name].sum()\n",
    "            task_seed_proportions = (solutions_on_pf / total_solutions_task_seed).fillna(0)\n",
    "\n",
    "            # Store the results for this task-seed combo\n",
    "            for method_name, proportion in task_seed_proportions.items():\n",
    "                all_proportions.append({\n",
    "                    task_column_name: task,\n",
    "                    'seed': seed, # Also store the seed\n",
    "                    'method_name': method_name,\n",
    "                    'metric_type': metric.replace('_', ' ').replace('normalized', '').strip().capitalize(),\n",
    "                    'proportion': proportion,\n",
    "                    'num_solutions': total_solutions_task_seed.get(method_name, 0)\n",
    "                })\n",
    "\n",
    "print(\"Averaging the per-task-seed results...\")\n",
    "per_task_seed_df = pd.DataFrame(all_proportions)\n",
    "\n",
    "# Filter out the 'Single-Best' method if it exists\n",
    "if 'Single-Best' in per_task_seed_df['method_name'].unique():\n",
    "    per_task_seed_df = per_task_seed_df[per_task_seed_df['method_name'] != 'Single-Best']\n",
    "\n",
    "# Forcefully remove the unused 'Single-Best' category to fix plotting issues\n",
    "if isinstance(per_task_seed_df['method_name'].dtype, pd.CategoricalDtype):\n",
    "    per_task_seed_df['method_name'] = per_task_seed_df['method_name'].cat.remove_unused_categories()\n",
    "\n",
    "# Determine the final sort order based on the new, more granular mean\n",
    "sort_order_df = per_task_seed_df.groupby('method_name', observed=True)['proportion'].mean().sort_values(ascending=False)\n",
    "sort_order = sort_order_df.index.tolist()\n",
    "# Note: n is now the average number of solutions per task-seed combination\n",
    "avg_solutions_per_method = per_task_seed_df.groupby('method_name')['num_solutions'].mean()\n",
    "\n",
    "print(\"Generating the final plot...\")\n",
    "plt.figure(figsize=(14, 10))\n",
    "ax = sns.barplot(\n",
    "    data=per_task_seed_df,\n",
    "    y='method_name',\n",
    "    x='proportion',\n",
    "    hue='metric_type',\n",
    "    order=sort_order,\n",
    "    palette='viridis',\n",
    "    errorbar='se'\n",
    ")\n",
    "\n",
    "new_yticklabels = []\n",
    "for method in sort_order:\n",
    "    avg_sol = avg_solutions_per_method.get(method, 0)\n",
    "    # The label now reflects that 'n' is the avg per task-seed run\n",
    "    new_yticklabels.append(f\"{method}\\n(n={avg_sol:.1f})\")\n",
    "\n",
    "ax.set_yticklabels(new_yticklabels)\n",
    "\n",
    "# Apply formatting\n",
    "ax.set_xlabel('Average Per-Task Ensemble Proportion on Global Pareto Front', fontsize=18)\n",
    "ax.set_ylabel('')\n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.legend(title='Cost Metric', fontsize=14, title_fontsize=16)\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "directory = \"../plots\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "plt.savefig(f\"{directory}/proportion_on_global_pf_per_task_seed_avg.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(f\"{directory}/proportion_on_global_pf_per_task_seed_avg.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Construction Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "objectives_to_plot = ['normalized_roc_auc_test', 'hw_score']\n",
    "\n",
    "# --- Performance: Subsample the data ---\n",
    "n_samples = 3000000\n",
    "if len(df) > n_samples:\n",
    "    print(f\"Dataset is large. Using a random sample of {n_samples} points for speed.\")\n",
    "    df_sample = df.sample(n=n_samples, random_state=42)\n",
    "else:\n",
    "    df_sample = df\n",
    "\n",
    "# --- Method Filtering/Sorting ---\n",
    "# ['Single-Best', 'Memory', 'QDO-ES', 'Ensemble Size', 'HAPEns', 'Multi-GES', 'GES*', 'Diskspace']\n",
    "custom_method_order = ['HAPEns', 'Multi-GES', 'Single-Best', 'QDO-ES', 'GES*']\n",
    "if custom_method_order:\n",
    "    method_names = [m for m in custom_method_order if m in df_sample['method_name'].unique()]\n",
    "else:\n",
    "    method_names = sorted(df_sample['method_name'].unique())\n",
    "\n",
    "num_methods = len(method_names)\n",
    "\n",
    "grid_cols = math.ceil(math.sqrt(num_methods))\n",
    "grid_rows = math.ceil(num_methods / grid_cols)\n",
    "\n",
    "# --- DYNAMIC FIGURE SIZE CALCULATION ---\n",
    "subplot_size_inch = 4 \n",
    "fig_width = grid_cols * subplot_size_inch\n",
    "fig_height = grid_rows * subplot_size_inch\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, axes = plt.subplots(\n",
    "    grid_rows, \n",
    "    grid_cols, \n",
    "    figsize=(fig_width, fig_height),\n",
    "    sharex=True, \n",
    "    sharey=True\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "print(\"Generating final polished density plots...\")\n",
    "\n",
    "# --- Loop through each method and create its subplot ---\n",
    "for i, method in enumerate(method_names):\n",
    "    ax = axes[i]\n",
    "    df_method_sample = df_sample[df_sample['method_name'] == method]\n",
    "    \n",
    "    sns.kdeplot(\n",
    "        data=df_method_sample,\n",
    "        x=objectives_to_plot[0],\n",
    "        y=objectives_to_plot[1],\n",
    "        fill=True,\n",
    "        alpha=0.5,\n",
    "        color='c',\n",
    "        levels=7,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    sns.kdeplot(\n",
    "        data=df_method_sample,\n",
    "        x=objectives_to_plot[0],\n",
    "        y=objectives_to_plot[1],\n",
    "        fill=False,\n",
    "        linewidths=1.0,\n",
    "        color='teal',\n",
    "        levels=7,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(method, fontsize=18, pad=12)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# --- Final Touches ---\n",
    "# Hide unused subplots\n",
    "for i in range(num_methods, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "# Set common labels for the entire figure\n",
    "fig.supxlabel('Normalized Test ROC AUC (inverted)', fontsize=22, y=0.07)\n",
    "fig.supylabel('Hardware Score', fontsize=22, x=0.08)\n",
    "\n",
    "# --- MODIFIED SECTION ---\n",
    "# Define the desired ticks\n",
    "major_ticks = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "# Set shared limits and ticks for all plots\n",
    "plt.setp(\n",
    "    axes, \n",
    "    xlim=(-0.02, 1.02), ylim=(-0.02, 1.02),\n",
    "    xticks=major_ticks, yticks=major_ticks\n",
    ")\n",
    "# --- END MODIFIED SECTION ---\n",
    "\n",
    "# Configure labels and ticks for all plots\n",
    "for i in range(num_methods):\n",
    "    ax = axes[i]\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0.07, 0.07, 1, 1])\n",
    "\n",
    "plt.savefig(\"../plots/density_plot.png\", dpi=300)\n",
    "plt.savefig(\"../plots/density_plot.pdf\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latex Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latex_table(df, repo, filename=\"table.tex\", max_char=15):\n",
    "    methods = df[\"method_name\"].unique()\n",
    "    task_ids = df[\"task_id\"].unique()\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"\\\\begin{longtable}{l\" + \"c\" * len(methods) + \"}\\n\")\n",
    "        f.write(\n",
    "            \"\\\\caption{Test ROC AUC - Binary: The mean and standard deviation of the test score over all folds for each method. The best methods per dataset are shown in bold. All methods close to the best method are considered best (using NumPy’s default \\\\texttt{isclose} function).}\\n\"\n",
    "        )\n",
    "        f.write(\"\\\\label{tab:results} \\\\\\\\ \\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\"Dataset & \" + \" & \".join(map(str, methods)) + \" \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\"\\\\endfirsthead\\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\"Dataset & \" + \" & \".join(map(str, methods)) + \" \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\"\\\\endhead\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\n",
    "            \"\\\\multicolumn{\"\n",
    "            + str(len(methods) + 1)\n",
    "            + \"}{r}{Continued on next page} \\\\\\\\\\n\"\n",
    "        )\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        f.write(\"\\\\endfoot\\n\")\n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\endlastfoot\\n\")\n",
    "\n",
    "        for task_id in task_ids:\n",
    "            dataset_name = repo.tid_to_dataset(\n",
    "                task_id\n",
    "            )  # Convert task_id to dataset name\n",
    "            truncated_name = (\n",
    "                (dataset_name[:max_char] + \"...\")\n",
    "                if len(dataset_name) > max_char\n",
    "                else dataset_name\n",
    "            )\n",
    "            escaped_name = truncated_name.replace(\"_\", \"\\\\_\")  # Escape underscores\n",
    "            line = [str(escaped_name)]  # Ensure the first item is a string\n",
    "            method_scores = []\n",
    "\n",
    "            for method in methods:\n",
    "                method_data = df[\n",
    "                    (df[\"task_id\"] == task_id) & (df[\"method_name\"] == method)\n",
    "                ]\n",
    "                if not method_data.empty:\n",
    "                    mean_score = method_data[\"roc_auc_test\"].mean()\n",
    "                    std_dev = method_data[\"roc_auc_test\"].std()\n",
    "                    score_str = f\"{mean_score:.4f}($\\\\pm${std_dev:.4f})\"\n",
    "                    method_scores.append((mean_score, score_str))\n",
    "                else:\n",
    "                    method_scores.append((None, \"-\"))\n",
    "\n",
    "            # Determine the best score\n",
    "            best_score = max(\n",
    "                score[0] for score in method_scores if score[0] is not None\n",
    "            )\n",
    "\n",
    "            for mean_score, score_str in method_scores:\n",
    "                if mean_score is not None and np.isclose(mean_score, best_score):\n",
    "                    line.append(f\"\\\\textbf{{{score_str}}}\")\n",
    "                else:\n",
    "                    line.append(score_str)\n",
    "\n",
    "            f.write(\" & \".join(line) + \" \\\\\\\\\\n\")\n",
    "\n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\end{longtable}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../tables\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "repo = load_repository(\"D244_F3_C1530_100\", cache=True)\n",
    "create_latex_table(df, repo, filename=\"../tables/table.tex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HA-ES",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
